@article{chen2021,
author 		= {Richerd J. Chen and Ming Y. Lu and Tiffany Y. CHen and Drew F. K; Williamson and Faisal Mahmood},
title 		= {Synthetic data in machine learning for medicine and healthcare},
journal 	= {Nature Biomedical Engineering},
volume 		= {5},
pages 		= {493-497},
year 		= {2021},
}

@article{xu2019,
author 		= {Chugui Xu and Ju Ren and Deyu Zhang and Yaoxue Zhang and Zhan Qin and Kui Ren},
title 		= {GANobfuscator: Mitigating Information Leakage Under GAN via Differential Privacy},
journal 	= {IEEE Transactions on Information Forensics and Security},
publisher = {IEEE},
volume 		= {14},
pages 		= {2358-2371},
year 		= {2019},
}

@article{wang2021,
author 		= {Zhenchen Wang and Puja Myles and Allan Tucker},
title 		= {Generating and evaluating cross-sectional synthetic electronic healthcare data: Preserving data utility and patient privacy},
journal 	= {Computational Intelligence},
publisher = {Willey},
volume 		= {37},
pages 		= {819-851},
year 		= {2021},
}

@article{yoon2020,
author 		= {Jinsung Yoon and Lydia N Drumright and Mihaela van der Schaar},
title 		= {Anonymization Through Data Synthesis Using Generative Adversarial Networks (ADS-GAN)},
journal 	= {IEEE J Biomed Health Inform},
publisher = {IEEE},
volume 		= {8},
pages 		= {2378-2388},
year 		= {2020},
}

@inproceedings{heyburn2018,
author 		= {Rachel Heyburn and Raymond R. Bond and Michaela Black and Maurice Mulvenna and Jonathan Wallace and Deborah Rankin and Brian Cleland},
title 		= {Machine learning using synthetic and real data: Similarity of evaluation metrics for different healthcare datasets and for different algorithms},
booktitle 	= {Data Science and Knowledge Engineering for Sensing Decision Support},
volume 		= {11},
pages 		= {1281-1291},
year 		= {2018},
}

@article{khaled2020,
author 		= {Khaled El Emam},
title 		= {Seven Ways to Evaluate the Utility of Synthetic Data},
journal 	= {IEEE Security & Privacy},
publisher = {IEEE},
volume 		= {18},
pages 		= {56-59},
year 		= {2020},
}

@article{review2022,
author 		= {Mikel Hernandez and Gorka Epelde and Ane Alberdi and Rodrigo Cilla and Debbie Rankin},
title 		= {Synthetic data generation for tabular health records: A systematic review},
journal 	= {Neurocomputing},
publisher = {Elsevier},
volume 		= {493},
pages 		= {28-45},
year 		= {2022},
}

@article{ctf,
author 		= {David Donoho},
title 		= {50 Years of Data Science},
journal 	= {JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS},
volume 		= {26:4},
pages 		= {745-766},
year 		= {2017},
}

@inproceedings{jpmorgan,
author 		= {Samuel A. Assefa and Danial Dervovic and Mahmoud Mahfouz and Robert E. Tillman and Prashant Reddy and Manueal Veloso},
title 		= {Generating synthetic data in finance: opportunities, challenges and pitfalls},
publisher = {Association for Computing Machinery},
pages 		= {1-8},
year 		= {2020},
}

@mastersthesis{evaluator2019,
author 		= {Bauke Brenninkmeijer},
title  		= {On the Generation and Evaluation of Tabular Data using GANs},
school 		= {Radboud University},
year   		= {2019}
}

@phdthesis{sdv,
author 		= {Andrew Montanez},
title  		= {SDV: An Open Source Library for Synthetic Data Generation},
school 		= {MASSACHUSETTS INSTITUTE OF TECHNOLOGY},
year   		= {2018}
}

% -- NOVAS REFERENCIAS

\@article{
    dcgan-improves-blood-cell-classifier,
    author 		= {Hartanto, C.A, et al},
    title 		= {DCGAN-generated Synthetic Images Effect on White Blood Cell Classification},
    journal 	= {IOP Conf. Ser.: Materials Science and Engineering},
    publisher   = {},
    volume 		= {1077},
    pages 		= {12-33},
    year 		= {2021},
}

\@article{diffusion-beats-gan,
    author = {Dhariwal, Prafulla and Nichol, Alex },
    title = {Diffusion Models Beat GANs on Image Synthesis},
    journal = {35th Conference on Neural Information Processing Systems (NeurIPS2021)},
    year = {2021}
}

\@article{
    breimanStatisticalModeling2001,
    author 		= {Breiman, Leo},
    title 		= {Statistical Modeling: The Two Cultures},
    journal 	= {Statistical Science},
    publisher   = {Institute of Mathematical Statistics},
    volume 		= {16},
    pages 		= {199-231},
    year 		= {2001},
}

\@article{
    underspecification,
    author 		= {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan et al},
    title 		= {Underspecification Presents Challenges for Credibility in
Modern Machine Learning},
    journal 	= {Journal of Machine Learning Research},    
    volume 		= {23},
    pages 		= {1-61},
    year 		= {2022},
}

== Nova revis√£o

@article{albahliEfficientGANbasedChest2020,
  title = {Efficient {{GAN-based Chest Radiographs}} ({{CXR}}) Augmentation to Diagnose Coronavirus Disease Pneumonia},
  author = {Albahli, Saleh},
  year = {2020},
  month = jun,
  journal = {International Journal of Medical Sciences},
  volume = {17},
  number = {10},
  pages = {1439--1448},
  issn = {1449-1907},
  doi = {10.7150/ijms.46684},
  urldate = {2024-05-09},
  abstract = {Background: As 2019 ends coronavirus disease start expanding all over the world. It is highly transmissible disease that can affect respiratory tract and can leads to organ failure. In 2020 it is declared by world health organization as ``Public health emergency of international concerns''. The current situation of Covid-19 and chest related diseases have already gone through radical change with the advancements of image processing tools. There is no effective method which can accurately identify all chest related diseases and tackle the multiple class problems with reliable results., Method: There are many potentially impactful applications of Deep Learning to fighting the Covid-19 from Chest X-Ray/CT Images, however, most are still in their early stages due to lack of data sharing as it continues to inhibit overall progress in a variety of medical research problems. Based on COVID-19 radiographical changes in CT images, this work aims to detect the possibility of COVID-19 in the patient. This work provides a significant contribution in terms of Gan based synthetic data and four different types of deep learning- based models which provided state of the art comparable results., Results: A Deep Neural Network model provides a significant contribution in terms of detecting COVID-19 and provides effective analysis of chest related diseases with respect to age and gender. Our model achieves 89\% accuracy in terms of Gan based synthetic data and four different types of deep learning- based models which provided state of the art comparable results., Conclusion: If the gap in identifying of all viral pneumonias is not filled with effective automation of chest disease detection the healthcare industry may have to bear unfavorable circumstances.},
  pmcid = {PMC7330663},
  pmid = {32624700},
  file = {C:\Users\maila\Zotero\storage\IQU2IUQZ\Albahli - 2020 - Efficient GAN-based Chest Radiographs (CXR) augmen.pdf}
}

@article{araujoSyntheticSARData2023,
  title = {Synthetic {{SAR Data Generator Using Pix2pix cGAN Architecture}} for {{Automatic Target Recognition}}},
  author = {Araujo, Gustavo F. and Machado, Renato and Pettersson, Mats I.},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {143369--143386},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3343910},
  urldate = {2024-05-14},
  abstract = {Synthetic Aperture Radar (SAR) technology has unique advantages but faces challenges in obtaining enough data for noncooperative target classes. We propose a method to generate synthetic SAR data using a modified pix2pix Conditional Generative Adversarial Networks (cGAN) architecture. The cGAN is trained to create synthetic SAR images with specific azimuth and elevation angles, demonstrating its capability to closely mimic authentic SAR imagery through convergence and collapsing analyses. The study uses a model-based algorithm to assess the practicality of the generated synthetic data for Automatic Target Recognition (ATR). The results reveal that the classification accuracy achieved with synthetic data is comparable to that attained with original data, highlighting the effectiveness of the proposed method in mitigating the limitations imposed by noncooperative SAR data scarcity for ATR. This innovative approach offers a promising solution to craft customized synthetic SAR data, ultimately enhancing ATR performance in remote sensing.},
  keywords = {Automatic target recognition,classification,Classification algorithms,conditional generative adversarial networks,data augmentation,Data augmentation,Generative adversarial networks,Generators,Neurons,Optical imaging,Pix2Pix,Radar polarimetry,synthetic aperture radar,Synthetic aperture radar,synthetic data,Target recognition,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\HNS3HCDZ\\Araujo et al. - 2023 - Synthetic SAR Data Generator Using Pix2pix cGAN Ar.pdf;C\:\\Users\\maila\\Zotero\\storage\\452CQVSP\\10363188.html}
}

@inproceedings{baoRareHeartTransplant2023,
  title = {Rare {{Heart Transplant Rejection Classification Using Diffusion-Based Synthetic Image Augmentation}}},
  booktitle = {2023 {{IEEE EMBS International Conference}} on {{Biomedical}} and {{Health Informatics}} ({{BHI}})},
  author = {Bao, Han and Deng, Jie and Xing, Shihao and Zhong, Yishan and Shi, Wenqi and Marteau, Benoit and Das, Bibhuti and Shehata, Bahig and Deshpande, Shriprasad and Wang, May D.},
  year = {2023},
  month = oct,
  pages = {1--4},
  publisher = {IEEE},
  address = {Pittsburgh, PA, USA},
  doi = {10.1109/BHI58575.2023.10313377},
  urldate = {2024-05-14},
  abstract = {Heart Transplant Rejection (HTR) is a rare condition that requires early detection to prevent lasting damage to the transplanted heart. Unfortunately, the current HTR grading through biopsy image classification lacks consistency among pathologists. In addition, it is a time-consuming task. In this work, we have developed an automated diagnosis pipeline to streamline the heart transplant histopathology image quantification and classification, in order to provide objectivity for clinical decision support for pathologists. Traditionally, developing an automated image classification requires a substantial amount of labeled data. However, HTR is a rare condition and the dataset is usually unbalanced. For example, the dataset from DNA Based Transplant Rejection (DTRT) comprises 1,509 rejection tile images and 190 times more non-rejection tile images. To address the small data sample challenge in training the classifiers, we developed a novel strategy that used diffusion model to generate synthetic images of rejection. We conducted comprehensive HTR grade classification comparing results using dataset with synthetic rejection tiles versus the dataset without any synthetic rejection tiles. The introduction of synthetic augmentation resulted in an improvement from 0.781 to 0.981 for sensitivity, and an improvement from 0.984 to over 0.998 in AUROC. This study illustrated that synthetic data augmentation is a feasible strategy in developing AI solutions for rare diseases. In the future, we will expand in this direction to benefit more rare disease clinical decision support development.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350310504},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\DU8XR9TS\Bao et al. - 2023 - Rare Heart Transplant Rejection Classification Usi.pdf}
}

@inproceedings{bhagatDataAugmentationUsing2019,
  title = {Data {{Augmentation}} Using {{Generative Adversarial Networks}} for {{Pneumonia}} Classification in Chest {{Xrays}}},
  booktitle = {2019 {{Fifth International Conference}} on {{Image Information Processing}} ({{ICIIP}})},
  author = {Bhagat, Vedant and Bhaumik, Swapnil},
  year = {2019},
  month = nov,
  pages = {574--579},
  issn = {2640-074X},
  doi = {10.1109/ICIIP47207.2019.8985892},
  urldate = {2024-05-09},
  abstract = {In medical images, data augmentation is essentially important for accurate classification of images especially when available data is limited. This paper proposes a noble data augmentation method of generating synthetic chest Xray images of patients with Pneumonia using Generative Adversarial Networks (GANs). The proposed model first uses conventional data augmentation techniques along with GANs to generate more training samples. A specific implementation of GANs allows us to produce unprecedented Chest X-Ray images of patients suffering from Pneumonia. The generated samples are then used to train a DCNN model to classify chest X-Ray images. The classifier significantly improves its accuracy after the introduction of synthetic data produced by the GAN model.},
  keywords = {data augmentation,deep convolutional neural network (DCNN),ehest X-ray,generative adversarial network (GAN),synthetic images},
  file = {C\:\\Users\\maila\\Zotero\\storage\\54KSSY4S\\Bhagat and Bhaumik - 2019 - Data Augmentation using Generative Adversarial Net.pdf;C\:\\Users\\maila\\Zotero\\storage\\HN9C8KI8\\8985892.html}
}

@inproceedings{buradLeveragingUnpairedImage2021,
  title = {Leveraging {{Unpaired Image}} to {{Image Translation}} for {{Generating High Quality Synthetic Data}}},
  booktitle = {2021 {{International Conference}} on {{Emerging Smart Computing}} and {{Informatics}} ({{ESCI}})},
  author = {Burad, Yash and Burad, Kushal},
  year = {2021},
  month = mar,
  pages = {98--101},
  doi = {10.1109/ESCI50559.2021.9396865},
  urldate = {2024-05-14},
  abstract = {Modern deep learning applications require huge data to deliver required accuracy. In most cases accumulating the required amount of data of sufficient quality is often very difficult and costly. We propose a GAN based solution to this problem. CycleGAN is a method of unpaired image -to -image translation which transforms the input image to target domain while preserving all the essential features of the input image. We can leverage CycleGAN to generate synthetic data of required quality by choosing the appropriate target domain.},
  keywords = {Computer Vision,CycleGAN,Deep learning,Gallium nitride,Generative adversarial networks,Generative Adversarial networks,Image to image translation,Informatics,Scene classification,Synthetic data generation,Transforms},
  file = {C\:\\Users\\maila\\Zotero\\storage\\LSURE235\\Burad and Burad - 2021 - Leveraging Unpaired Image to Image Translation for.pdf;C\:\\Users\\maila\\Zotero\\storage\\IMRWEVW8\\9396865.html}
}

@article{deptoQuantifyingImbalancedClassification2023a,
  title = {Quantifying Imbalanced Classification Methods for Leukemia Detection},
  author = {Depto, Deponker Sarker and Rizvee, Md. Mashfiq and Rahman, Aimon and Zunair, Hasib and Rahman, M. Sohel and Mahdy, M. R. C.},
  year = {2023},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {152},
  pages = {106372},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2022.106372},
  urldate = {2024-05-22},
  abstract = {Uncontrolled proliferation of B-lymphoblast cells is a common characterization of Acute Lymphoblastic Leukemia (ALL). B-lymphoblasts are found in large numbers in peripheral blood in malignant cases. Early detection of the cell in bone marrow is essential as the disease progresses rapidly if left untreated. However, automated classification of the cell is challenging, owing to its fine-grained variability with B-lymphoid precursor cells and imbalanced data points. Deep learning algorithms demonstrate potential for such fine-grained classification as well as suffer from the imbalanced class problem. In this paper, we explore different deep learning-based State-Of-The-Art (SOTA) approaches to tackle imbalanced classification problems. Our experiment includes input, GAN (Generative Adversarial Networks), and loss-based methods to mitigate the issue of imbalanced class on the challenging C-NMC and ALLIDB-2 dataset for leukemia detection. We have shown empirical evidence that loss-based methods outperform GAN-based and input-based methods in imbalanced classification scenarios.},
  keywords = {Adversarial training,Domain adaptation,Imbalanced classification,Leukemia classification},
  file = {C\:\\Users\\maila\\Zotero\\storage\\TYAC72R8\\Depto et al. - 2023 - Quantifying imbalanced classification methods for .pdf;C\:\\Users\\maila\\Zotero\\storage\\A55M8YHP\\S0010482522010800.html}
}

@inproceedings{dimitrakopoulosISINGGANAnnotatedData2020,
  title = {{{ISING-GAN}}: {{Annotated Data Augmentation}} with a {{Spatially Constrained Generative Adversarial Network}}},
  shorttitle = {{{ISING-GAN}}},
  booktitle = {2020 {{IEEE}} 17th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Dimitrakopoulos, P. and Sfikas, G. and Nikou, C.},
  year = {2020},
  month = apr,
  pages = {1600--1603},
  publisher = {IEEE},
  address = {Iowa City, IA, USA},
  doi = {10.1109/ISBI45749.2020.9098618},
  urldate = {2024-05-13},
  abstract = {Data augmentation is a popular technique with which new dataset samples are artificially synthesized to the end of assisting training of learning-based algorithms and avoiding overfitting. Methods based on generative adversarial networks (GANs) have recently rekindled interest in research on new techniques for data augmentation. With the current paper we propose a new GAN-based model for data augmentation, comprising a suitable Markov random field-based spatial constraint that encourages synthesis of spatially smooth outputs. Oriented towards use with medical imaging sets where a localization/segmentation annotation is available, our model can simultaneously also produce artificial annotations. We gauge performance numerically by measuring performance through U-Net trained to detect cells on microscopy images, by taking into account the produced augmented dataset. Numerical trials, as well as qualitative results validate the contribution of our model.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-5386-9330-8},
  langid = {english},
  keywords = {Lido},
  file = {C:\Users\maila\Zotero\storage\TZPTM7P5\Dimitrakopoulos et al. - 2020 - ISING-GAN Annotated Data Augmentation with a Spat.pdf}
}

@inproceedings{divyaMedicalMRImage2022,
  title = {Medical {{MR Image Synthesis}} Using {{DCGAN}}},
  booktitle = {2022 {{First International Conference}} on {{Electrical}}, {{Electronics}}, {{Information}} and {{Communication Technologies}} ({{ICEEICT}})},
  author = {Divya, S and Suresh, L Padma and John, Ansamma},
  year = {2022},
  month = feb,
  pages = {01--04},
  doi = {10.1109/ICEEICT53079.2022.9768647},
  urldate = {2024-05-14},
  abstract = {Generative Adversarial Networks (GANs) have been extensively gained considerable attention since 2014. Irrefutably saying, their most remarkable success has been made in domains such as computer vision and medical image processing. Despite the noteworthy success attained to date, applying GANs to real world problems still posses significant challenges, one among which is diversity of image generation and detection of fake images from real ones. Focusing on the extend to which various GAN models have made headway against these challenges, this study provides an overview of DCGAN architecture and its application as a synthetic data generator and act an a binary classifier, which detects real or fake images using brain tumorous Magnetic Resonance Imaging (MRI) dataset.},
  keywords = {brain tumor,Computational modeling,Computer architecture,Computer vision,Focusing,GAN,Generative adversarial networks,Image synthesis,Magnetic resonance imaging,medical image processing,MRI,synthetic data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\C6MZU83H\\Divya et al. - 2022 - Medical MR Image Synthesis using DCGAN.pdf;C\:\\Users\\maila\\Zotero\\storage\\C2UKIBLL\\9768647.html}
}

@article{eshunDeepConvolutionalNeural2024,
  title = {A Deep Convolutional Neural Network for the Classification of Imbalanced Breast Cancer Dataset},
  author = {Eshun, Robert B. and Bikdash, Marwan and Islam, A. K. M. Kamrul},
  year = {2024},
  month = jun,
  journal = {Healthcare Analytics},
  volume = {5},
  pages = {100330},
  issn = {2772-4425},
  doi = {10.1016/j.health.2024.100330},
  urldate = {2024-05-22},
  abstract = {The primary procedures for breast cancer diagnosis involve the assessment of histopathological slide images by skilled patholo-gists. This procedure is prone to human subjectivity and can lead to diagnostic errors with adverse implications for patient health and welfare. Artificial intelligence-based models have yielded promising results in other medical tasks and offer tools for potentially addressing the shortcomings of traditional medical image analysis. The BreakHis breast cancer dataset suffers from insufficient data for the minority class with an imbalance ratio {$>$}0.40, which poses challenges for deep learning models. To avoid performance degradation, researchers have explored a variety of data augmentation schemes to generate adequate samples for analysis. This study designed a Deep Convolutional Neural Network (DCGAN) with specific generator and discriminator architectures to mitigate model instability and generate high-quality synthetic data for the minority class. The balanced dataset was passed to the fine-tuned ResNet50 model for breast tumor detection. The study produced high accuracy in diagnosing benign/malignancy at 40X magnification, outperforming the state-of-art. The results demonstrated that deep learning methods can potentially to support effective screening in clinical practice.},
  keywords = {BreakHis dataset,Breast cancer histopathological images,Data augmentation,Deep convolutional generative adversarial network,Deep convolutional neural network,Frechet inception distance evaluation criterion},
  file = {C\:\\Users\\maila\\Zotero\\storage\\Y4QVWWVE\\Eshun et al. - 2024 - A deep convolutional neural network for the classi.pdf;C\:\\Users\\maila\\Zotero\\storage\\V48ICW5D\\S2772442524000327.html}
}

@inproceedings{farooqProofofConceptTechniquesGenerating2021,
  title = {Proof-of-{{Concept Techniques}} for {{Generating Synthetic Thermal Facial Data}} for {{Training}} of {{Deep Learning Models}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Consumer Electronics}} ({{ICCE}})},
  author = {Farooq, Muhammad Ali and Corcoran, Peter},
  year = {2021},
  month = jan,
  pages = {1--6},
  issn = {2158-4001},
  doi = {10.1109/ICCE50685.2021.9427690},
  urldate = {2024-05-09},
  abstract = {Thermal imaging has played a dynamic role in the diversified field of consumer technology applications. To build artificially intelligent thermal imaging systems, large scale thermal datasets are required for successful convergence of complex deep learning models. In this study, we have highlighted various techniques for generating large scale synthetic facial thermal data using both public and locally gathered datasets. It includes data augmentation, synthetic data generation using StyleGAN network, and 2D to 3D image reconstruction using deep learning architectures. Training and validation accuracy of Wide ResNet CNN for binary gender recognition task is improved by 4.6\% and 4.4\% using original and newly generated synthetic data with an overall test accuracy of 83.33\%.},
  keywords = {Augmentation,Deep learning,Deep Neural Networks,Face recognition,Feature extraction,GAN,Imaging,Infrared Imaging,LWIR,Neural networks,Synthetic Data,Three-dimensional displays,Training,Wide ResNet},
  file = {C:\Users\maila\Zotero\storage\GGHKZ6MR\Farooq and Corcoran - 2021 - Proof-of-Concept Techniques for Generating Synthet.pdf}
}

@article{ferrer-sanchezPredictionRiskCancer2022,
  title = {Prediction of the Risk of Cancer and the Grade of Dysplasia in Leukoplakia Lesions Using Deep Learning},
  author = {{Ferrer-S{\'a}nchez}, Antonio and Bagan, Jose and {Vila-Franc{\'e}s}, Joan and {Magdalena-Benedito}, Rafael and {Bagan-Debon}, Leticia},
  year = {2022},
  month = sep,
  journal = {Oral Oncology},
  volume = {132},
  pages = {105967},
  issn = {1368-8375},
  doi = {10.1016/j.oraloncology.2022.105967},
  urldate = {2024-05-22},
  abstract = {Objectives To estimate the probability of malignancy of an oral leukoplakia lesion using Deep Learning, in terms of evolution to cancer and high-risk dysplasia. Materials and Methods A total of 261 oral leukoplakia lesions with a mean of 5.5~years follow-up were analysed from standard digital photographs. A deep learning pipeline composed by a U-Net based segmentation of the lesion followed by a multi-task CNN classifier was used to predict the malignant transformation and the risk of dysplasia of the lesion. An explainability heatmap is constructed using LIME in order to interpret the decision of the model for each output. Results A Dice coefficient of 0.561 was achieved on the segmentation task. For the prediction of a malignant transformation, the model provided a sensitivity of 1 with a specificity of 0.692. For the prediction of high-risk dysplasia, the model achieved a specificity of 0.740 and a sensitivity of 0.928. Conclusion The proposed model using deep learning can be a helpful tool for predicting the possible malignant evolution of oral leukoplakias. The generated heatmap provides a high confidence on the output of the model and enables its interpretability.},
  keywords = {Deep Learning,Dysplasia,Malignant transformation,Oral Leukoplakia},
  file = {C:\Users\maila\Zotero\storage\G9EWCNIW\Ferrer-S√°nchez et al. - 2022 - Prediction of the risk of cancer and the grade of .pdf}
}

@inproceedings{fooImageDataAugmentation2022,
  title = {Image {{Data Augmentation}} with {{Unpaired Image-to-Image Camera Model Translation}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Foo, Chi Fa and Winkler, Stefan},
  year = {2022},
  month = oct,
  pages = {3246--3250},
  publisher = {IEEE},
  address = {Bordeaux, France},
  doi = {10.1109/ICIP46576.2022.9897671},
  urldate = {2024-05-13},
  abstract = {Many image datasets are built from web searches, with images taken by various cameras. The variance of camera sources can lead to different camera signals and colors within images of the same class, which may impede neural networks from fitting the data. To generalize neural networks to different camera sources, we propose an augmentation method using unpaired image-to-image translation to transfer training images into another camera model domain. Our approach utilizes CycleGAN to create a translation mapping between two different camera models. We show that such a mapping can be applied to any image as a form of data augmentation and is able to outperform traditional color-based transformations. Additionally, this approach can be further enhanced with geometric transformations.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66549-620-9},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\8NW9CLFS\Foo and Winkler - 2022 - Image Data Augmentation with Unpaired Image-to-Ima.pdf}
}

@misc{gaoDiffGuardSemanticMismatchGuided2023,
  title = {{{DiffGuard}}: {{Semantic Mismatch-Guided Out-of-Distribution Detection}} Using {{Pre-trained Diffusion Models}}},
  shorttitle = {{{DiffGuard}}},
  author = {Gao, Ruiyuan and Zhao, Chenchen and Hong, Lanqing and Xu, Qiang},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07687},
  eprint = {2308.07687},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-14},
  abstract = {Given a classifier, the inherent property of semantic Outof-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to IMAGENET-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\maila\Zotero\storage\VRNE9NUX\Gao et al. - 2023 - DiffGuard Semantic Mismatch-Guided Out-of-Distrib.pdf}
}

@article{giusteExplainableSyntheticImage2023a,
  title = {Explainable Synthetic Image Generation to Improve Risk Assessment of Rare Pediatric Heart Transplant Rejection},
  author = {Giuste, Felipe O. and Sequeira, Ryan and Keerthipati, Vikranth and Lais, Peter and Mirzazadeh, Ali and Mohseni, Arshawn and Zhu, Yuanda and Shi, Wenqi and Marteau, Benoit and Zhong, Yishan and Tong, Li and Das, Bibhuti and Shehata, Bahig and Deshpande, Shriprasad and Wang, May D.},
  year = {2023},
  month = mar,
  journal = {Journal of Biomedical Informatics},
  volume = {139},
  pages = {104303},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2023.104303},
  urldate = {2024-05-22},
  abstract = {Expert microscopic analysis of cells obtained from frequent heart biopsies is vital for early detection of pediatric heart transplant rejection to prevent heart failure. Detection of this rare condition is prone to low levels of expert agreement due to the difficulty of identifying subtle rejection signs within biopsy samples. The rarity of pediatric heart transplant rejection also means that very few gold-standard images are available for developing machine learning models. To solve this urgent clinical challenge, we developed a deep learning model to automatically quantify rejection risk within digital images of biopsied tissue using an explainable synthetic data augmentation approach. We developed this explainable AI framework to illustrate how our progressive and inspirational generative adversarial network models distinguish between normal tissue images and those containing cellular rejection signs. To quantify biopsy-level rejection risk, we first detect local rejection features using a binary image classifier trained with expert-annotated and synthetic examples. We converted these local predictions into a biopsy-wide rejection score via an interpretable histogram-based approach. Our model significantly improves upon prior works with the same dataset with an area under the receiver operating curve (AUROC) of 98.84\% for the local rejection detection task and 95.56\% for the biopsy-rejection prediction task. A biopsy-level sensitivity of 83.33\% makes our approach suitable for early screening of biopsies to prioritize expert analysis. Our framework provides a solution to rare medical imaging challenges currently limited by small datasets.},
  keywords = {Cardiac pathology,Explainable artificial intelligence,Image classification,Image generation,Multiple instance learning,Whole-slide imaging},
  file = {C:\Users\maila\Zotero\storage\7MLE43UU\Giuste et al. - 2023 - Explainable synthetic image generation to improve .pdf}
}

@inproceedings{guoSARImageData2022,
  title = {{{SAR Image Data Augmentation}} via {{Residual}} and {{Attention-Based Generative Adversarial Network}} for {{Ship Detection}}},
  booktitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Guo, Yu-Shi and Li, Heng-Chao and Hu, Wen-Shuai and Wang, Wei-Ye},
  year = {2022},
  month = jul,
  pages = {439--442},
  issn = {2153-7003},
  doi = {10.1109/IGARSS46834.2022.9884798},
  urldate = {2024-05-14},
  abstract = {In recent years, generative adversarial networks (GANs) have been successfully applied to generate the SAR images. However, due to the fact that it is more difficult to generate the images than to distinguish the real or fake, GANs usually suffer from the problems of unstable training and mode collapse. As such, a residual and attention-based generative adversarial network (RAGAN) is proposed for SAR data augmentation. Firstly, the directional bounding box is used as a constraint in the RAGAN to limit the position of ship in the generated SAR image, which can be further set as the annotation of the SAR image for ship detection directly. After that, inspired by the residual and attention learning, a residual and attention block (RABlock) and a transposed RABlock (TRABlock) are designed to improve the generator of the RAGAN, thus preventing the whole model from gradient vanishing and suppressing the effects of speckle noise and background to enhance the quality of the generated SAR images. Experimental results on the HRSID data set demonstrate the effectiveness of our RAGAN model in SAR data augmentation for ship detection.},
  keywords = {Annotations,attention mechanism,data augmentation,Data models,generative ad-versarial networks,Generative adversarial networks,Generators,Radar polarimetry,residual learning,Speckle,Synthetic aperture radar,target detection,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\YWFHGZ9W\\Guo et al. - 2022 - SAR Image Data Augmentation via Residual and Atten.pdf;C\:\\Users\\maila\\Zotero\\storage\\U344ISM8\\9884798.html}
}

@article{hanCombiningNoisetoImageImagetoImage2019,
  title = {Combining {{Noise-to-Image}} and {{Image-to-Image GANs}}: {{Brain MR Image Augmentation}} for {{Tumor Detection}}},
  shorttitle = {Combining {{Noise-to-Image}} and {{Image-to-Image GANs}}},
  author = {Han, Changhee and Rundo, Leonardo and Araki, Ryosuke and Nagano, Yudai and Furukawa, Yujiro and Mauri, Giancarlo and Nakayama, Hideki and Hayashi, Hideaki},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {156966--156977},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2947606},
  urldate = {2024-05-13},
  abstract = {Convolutional Neural Networks (CNNs) achieve excellent computer-assisted diagnosis with sufficient annotated training data. However, most medical imaging datasets are small and fragmented. In this context, Generative Adversarial Networks (GANs) can synthesize realistic/diverse additional training images to fill the data lack in the real image distribution; researchers have improved classification by augmenting data with noise-to-image (e.g., random noise samples to diverse pathological images) or imageto-image GANs (e.g., a benign image to a malignant one). Yet, no research has reported results combining noise-to-image and image-to-image GANs for further performance boost. Therefore, to maximize the DA effect with the GAN combinations, we propose a two-step GAN-based DA that generates and refines brain Magnetic Resonance (MR) images with/without tumors separately: (i) Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for high-resolution MR image generation, first generates realistic/diverse 256{\texttimes}256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT) that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused GAN loss, further refines the texture/shape of the PGGAN-generated images similarly to the real ones. We thoroughly investigate CNN-based tumor classification results, also considering the influence of pre-training on ImageNet and discarding weird-looking GAN-generated images. The results show that, when combined with classic DA, our two-step GAN-based DA can significantly outperform the classic DA alone, in tumor detection (i.e., boosting sensitivity 93.67\% to 97.48\%) and also in other medical imaging tasks.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\MJU8RT2D\Han et al. - 2019 - Combining Noise-to-Image and Image-to-Image GANs .pdf}
}

@inproceedings{hostinContextGanControllableContext2023,
  title = {Context-{{Gan}}: {{Controllable Context Image Generation Using Gans}}},
  shorttitle = {Context-{{Gan}}},
  booktitle = {2023 {{IEEE}} 20th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Hostin, Marc-Adrien and Sivtsov, Vladimir and Attarian, Shahram and Bendahan, David and Bellemare, Marc-Emmanuel},
  year = {2023},
  month = apr,
  pages = {1--5},
  publisher = {IEEE},
  address = {Cartagena, Colombia},
  doi = {10.1109/ISBI53787.2023.10230602},
  urldate = {2024-05-14},
  abstract = {We propose an enhancement to label-to-image GANs. Based on a Pix2Pix architecture, ConText-GAN allows generating images in a controlled way. Given a feature map as input, ConText-GAN can generate images with a specified layout and label content. As an application, ConText-GAN is used to perform a more realistic than usual data augmentation from an MRI dataset. We show the validity of the generated images with respect to the input feature maps. The relevance of the approach is demonstrated by the improvement of the segmentation result following a data augmentation performed with ConText-GAN compared to classical methods. A practical application is presented in the challenging context of U-Net segmentation of MRI of fat infiltrated muscles.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66547-358-3},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\5LI5R98Y\Hostin et al. - 2023 - Context-Gan Controllable Context Image Generation.pdf}
}

@inproceedings{hungAugmentationSmallTraining2021,
  title = {Augmentation of {{Small Training Data Using GANs}} for {{Enhancing}} the {{Performance}} of {{Image Classification}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Hung, Shih-Kai and Gan, John Q.},
  year = {2021},
  month = jan,
  pages = {3350--3356},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412399},
  urldate = {2024-05-14},
  abstract = {It is difficult to achieve high performance without sufficient training data for deep convolutional neural networks (DCNNs) to learn. Data augmentation plays an important role in improving robustness and preventing overfitting in machine learning for many applications such as image classification. In this paper, a novel method for data augmentation is proposed to solve the problem of machine learning with small training datasets. The proposed method can synthesise similar images with rich diversity from only a single original training sample to increase the number of training data by using generative adversarial networks (GANs). It is expected that the synthesised images possess class-informative features, which may be in the validation or testing data but not in the training data due to that the training dataset is small, and thus they can be effective as augmented training data to improve the classification accuracy of DCNNs. The experimental results have demonstrated that the proposed method with a novel GAN framework for image training data augmentation can significantly enhance the classification performance of DCNNs for applications where original training data is limited.},
  keywords = {data augmentation,DCNNs,GANs,Generative adversarial networks,image classification,Machine learning,Neural networks,Pattern recognition,Robustness,Training,Training data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\RZGD744G\\Hung and Gan - 2021 - Augmentation of Small Training Data Using GANs for.pdf;C\:\\Users\\maila\\Zotero\\storage\\YFT7NYHE\\9412399.html}
}

@inproceedings{hwangImageDataAugmentation2021,
  title = {Image {{Data Augmentation}} for {{SAR Automatic Target Recognition Using TripleGAN}}},
  booktitle = {2021 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  author = {Hwang, Jieon and Shin, Yoan},
  year = {2021},
  month = oct,
  pages = {312--314},
  publisher = {IEEE},
  address = {Jeju Island, Korea, Republic of},
  doi = {10.1109/ICTC52510.2021.9621194},
  urldate = {2024-05-13},
  abstract = {Synthetic aperture radar (SAR) images can be observed in all weather conditions. In addition, SAR automatic target recognition (ATR) is an important technology in the field of remote sensing image analysis. However, it is expensive to acquire SAR images, which limits the construction of additional datasets. In this paper, we propose a method to enhance ATR performance of the corresponding SAR images based on triple-generative adversarial networks (GANs). We added a classifier to learn together so that generator converges into the real data distribution. Experiments on the MSTAR dataset confirmed that the proposed model is applicable through the classification model.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66542-383-0},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\XQITNZ56\Hwang and Shin - 2021 - Image Data Augmentation for SAR Automatic Target R.pdf}
}

@inproceedings{hydockGenerationSyntheticData2023,
  title = {Generation of {{Synthetic Data}} for {{Medical Decision Support Applications}}},
  booktitle = {2023 {{IEEE Applied Imagery Pattern Recognition Workshop}} ({{AIPR}})},
  author = {Hydock, Kenneth and Elliott, Andrea and Busch, Mike and Lipchak, Lauren and Blair, Daniel and Chapman, John},
  year = {2023},
  month = sep,
  pages = {1--7},
  publisher = {IEEE Computer Society},
  doi = {10.1109/AIPR60534.2023.10440689},
  urldate = {2024-05-14},
  abstract = {Computer vision has the potential to accelerate decision support in a variety of medical applications, but a paucity of high-quality, open-source datasets hinders the development of decision support applications for open medical environments (i.e., non-laparoscopic). Synthetic data holds promise as a solution for difficult-to-obtain data, such as images from high stress environments or complicated by privacy concerns associated with medical imagery. Modern applications of synthetic data generation such as digital twins (DT) can create high-fidelity and photo-realistic images the surpass traditional data augmentation practices. Moreover, synthetic data is cost-effective, efficient, and highly scalable after initial creation of the assets and environment. This study presents a framework to synthetically generate annotated images from digital 3-D assets in random perspectives and orientations for use in computer-vision (CV) for open medical applications. Datasets of 1,000 annotated synthetic images and 681 real-world images of six object classes were employed in a transfer learning toolkit for an experiment to determine the feasibility of utilizing synthetic data to train models on CV applications to augment or replace training with real-world data. The experiment tested model performance based on three datasets: 1. Synthetic data only; 2. Real-world data only; 3. Training with synthetic data for evaluation with real-world data. Results showed that training on synthetic data to evaluate real-world data met or exceeded the performance of training on real-world data in four of six classes within the datasets utilized. Results show promise for utilizing synthetic data as an alternative to costly, time consuming, and difficult to obtain data types with many areas for further study, such as more detailed and comprehensive environments and assets as well as methodology for noise injection to improve model performance.},
  isbn = {9798350359527},
  langid = {english}
}

@inproceedings{ivanovsSyntheticImageGeneration2023,
  title = {Synthetic {{Image Generation With}} a {{Fine-Tuned Latent Diffusion Model}} for {{Organ}} on {{Chip Cell Image Classification}}},
  booktitle = {2023 {{Signal Processing}}: {{Algorithms}}, {{Architectures}}, {{Arrangements}}, and {{Applications}} ({{SPA}})},
  author = {Ivanovs, Maksims and Leja, Laura and Zviedris, K{\=a}rlis and Rimsa, Roberts and Narbute, Karina and Movcana, Valerija and Rumnieks, Felikss and Strods, Arnis and Gillois, Kevin and Mozolevskis, Gatis and Abols, Arturs and Kadikis, Roberts},
  year = {2023},
  month = sep,
  pages = {148--153},
  issn = {2326-0319},
  doi = {10.23919/SPA59660.2023.10274460},
  urldate = {2024-05-14},
  abstract = {Augmentation of the datasets of authentic microscopy images with synthetic images is a promising solution to the problem of the limited availability of biomedical data for training deep neural network (DNN) based classifiers. In the present study, we use a text-to-image latent stable diffusion model fine-tuned by means of low-rank adaptation (LoRA) to augment a small dataset of the images of organ on chip cells. While the resulting synthetic images appear quite similar to the authentic images on which the low-rank adaptation was performed, we find that neither training the EfficientNetB7 DNN model solely on the synthetic data nor augmentation of the real-world dataset with different proportions (10, 25, 50, and 75 percent) of these data leads to the improvement of the accuracy of the model. The findings of our study suggest that a further exploration of the low-rank adaptation options is needed to fully use the capacity of latent diffusion models for the synthesis of biomedical images.},
  keywords = {Adaptation models,Artificial neural networks,Biological system modeling,Image synthesis,Microscopy,Signal processing algorithms,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\GDNSDNLA\\Ivanovs et al. - 2023 - Synthetic Image Generation With a Fine-Tuned Laten.pdf;C\:\\Users\\maila\\Zotero\\storage\\6XTFYZVK\\10274460.html}
}

@article{kimGANBasedSyntheticData2022,
  title = {{{GAN-Based Synthetic Data Augmentation}} for {{Infrared Small Target Detection}}},
  author = {Kim, Jun-Hyung and Hwang, Youngbae},
  year = {2022},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {60},
  pages = {1--12},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2022.3179891},
  urldate = {2024-05-13},
  abstract = {Recently, convolutional neural networks (CNNs) have achieved state-of-the-art performance in infrared small target detection. However, the limited number of public training data restricts the performance improvement of CNN-based methods. To handle the scarcity of training data, we propose a method that can generate synthetic training data for infrared small target detection. We adopt the generative adversarial network framework where synthetic background images and infrared small targets are generated in two independent processes. In the first stage, we synthesize infrared images by transforming visible images into infrared ones. In the second stage, target masks are implanted on the transformed images. Then, the proposed intensity modulation network synthesizes realistic target objects that can be diversely generated from further image processing. Experimental results on the recent public dataset show that, when we train various detection networks using the dataset composed of both real and synthetic images, detection networks yield better performance than using real data only.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\8DTY48IF\Kim and Hwang - 2022 - GAN-Based Synthetic Data Augmentation for Infrared.pdf}
}

@inproceedings{kongSARTargetRecognition2021,
  title = {{{SAR Target Recognition}} with {{Generative Adversarial Network}} ({{GAN}})-Based {{Data Augmentation}}},
  booktitle = {2021 13th {{International Conference}} on {{Advanced Infocomm Technology}} ({{ICAIT}})},
  author = {Kong, Jiayuan and Zhang, Fangzheng},
  year = {2021},
  month = oct,
  pages = {215--218},
  publisher = {IEEE},
  address = {Yanji, China},
  doi = {10.1109/ICAIT52638.2021.9701974},
  urldate = {2024-05-13},
  abstract = {In convolutional neural network (CNN)-based synthetic aperture radar (SAR) target recognition, a large number of SAR image datasets are required to train the CNN. To deal with the problem of insufficient training datasets, a generative adversarial network (GAN) can be applied for data augmentation. In this paper, the performance of SAR target recognition with GAN-based data augmentation is investigated using the MSTAR datasets. The results show that, by applying the GAN-based data augmentation using a proper data augmentation factor, a target recognition accuracy as high as 99.38\% can be achieved, which is improved by 2.35\% compared to the condition without data augmentation. Thus, the GAN-based data augmentation provides a promising solution to enhancing the accuracy of SAR target recognition.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66543-188-0},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\5AD8XG87\Kong and Zhang - 2021 - SAR Target Recognition with Generative Adversarial.pdf}
}

@inproceedings{kukrejaGANbasedSyntheticData2020,
  title = {{{GAN-based}} Synthetic Data Augmentation for Increased {{CNN}} Performance in {{Vehicle Number Plate Recognition}}},
  booktitle = {2020 4th {{International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Kukreja, Vinay and Kumar, Deepak and Kaur, Amandeep and {Geetanjali} and {Sakshi}},
  year = {2020},
  month = nov,
  pages = {1190--1195},
  doi = {10.1109/ICECA49313.2020.9297625},
  urldate = {2024-05-14},
  abstract = {In today's modern era, parking remains a big problem for a lot of people. This problem consumes a person individual's time by finding the right spot for parking. In the current research, the concept of an automatic parking system using the vehicle license plate or the number plate recognition is discussed. It will improve the process with much less hassle by removing human interaction. It will also lead to advancement in the security of vehicles evading the requirement of a slip or a magnetic card which is used to goes in and out for registering vehicles in a parking place. The researcher uses image processing algorithms to make an entry in the database of the parking automatically. AVNPR (Automatic Vehicle Number Plate Recognition) is used for the identification of the number of plates. Due to noise issues, deep algorithms like CNN (convolutional neural networks), RNN (recurrent neural networks) do not correctly recognize the miss-identification of the numbers in the vehicle plate. This problem is rectified by the authors by using the GAN (Generative adversarial networks) algorithm. GAN helps to create high-resolution images from a single low-resolution image. After applying the GAN, the classification of the vehicle plate is done through CNN. During experimentation, the proposed approach achieves 99.39\% recognition accuracy for a vehicle number plate. Hence, the proposed system is suitable for identifying the numbers in the vehicle number plate automatically. Moreover proposed system compared with existing models, it has been found that it has achieved higher accuracy than the other models.},
  keywords = {Cameras,Convolutional Neural Network,Cross-Entropy,Data Augmentation,Gallium nitride,Generative adversarial networks,Generators,Image processing,Stochastic gradient descent,Testing,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\74ZDZ73M\\Kukreja et al. - 2020 - GAN-based synthetic data augmentation for increase.pdf;C\:\\Users\\maila\\Zotero\\storage\\B89DPC62\\9297625.html}
}

@inproceedings{kumpatlaRevolutionizingBrainTumour2023,
  title = {Revolutionizing {{Brain Tumour Prediction}}: {{A Pioneering GAN-based Framework}} for {{Synthetic Data Generation}}},
  shorttitle = {Revolutionizing {{Brain Tumour Prediction}}},
  booktitle = {2023 7th {{International Conference}} on {{I-SMAC}} ({{IoT}} in {{Social}}, {{Mobile}}, {{Analytics}} and {{Cloud}}) ({{I-SMAC}})},
  author = {Kumpatla, Gautam and Veresi, Hemanth and P, Bala Subrahmanyam and Abhishek, S and T, Anjali},
  year = {2023},
  month = oct,
  pages = {548--553},
  publisher = {IEEE},
  address = {Kirtipur, Nepal},
  doi = {10.1109/I-SMAC58438.2023.10290627},
  urldate = {2024-05-14},
  abstract = {Medical analysis is a key component of contemporary healthcare since it helps doctors diagnose patients accurately to plan and track their treatments. Accurate identification of brain tumors is essential for doctors to treat patients swiftly and efficiently. This research investigates a novel approach to overcome the constraints imposed by scarce and sensitive medical data, generating synthetic images of brain tumors using Generative Adversarial Networks (GANs). The proposed method has the potential to enhance activities involving medical processing, which will aid in making diagnoses and formulating treatment plans. The preliminary findings demonstrate its potential relevance to a wider variety of medical processing tasks, demonstrating that this augmentation method yields significant benefits.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350341485},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\959YGVZN\Kumpatla et al. - 2023 - Revolutionizing Brain Tumour Prediction A Pioneer.pdf}
}

@inproceedings{linMedicalDataAugmentation2019,
  title = {Medical {{Data Augmentation Using Generative Adversarial Networks}} : {{X-ray Image Generation}} for {{Transfer Learning}} of {{Hip Fracture Detection}}},
  shorttitle = {Medical {{Data Augmentation Using Generative Adversarial Networks}}},
  booktitle = {2019 {{International Conference}} on {{Technologies}} and {{Applications}} of {{Artificial Intelligence}} ({{TAAI}})},
  author = {Lin, Ying-Jia and Chung, I-Fang},
  year = {2019},
  month = nov,
  pages = {1--5},
  publisher = {IEEE},
  address = {Kaohsiung, Taiwan},
  doi = {10.1109/TAAI48200.2019.8959908},
  urldate = {2024-05-13},
  abstract = {In the medical domain, it is difficult to retrieve large datasets. Classic data augmentation methods such as flipping, rotating, and scaling, are helpful in classification tasks, but these methods usually are not good enough to increase diversities and variances in small datasets. In this study, we applied AC-GAN (Auxiliary Classifier GANs) for data augmentation of our Limb X-ray dataset. We pre-trained the model for hip fracture detection of our Pelvic X-ray dataset by utilizing transfer learning with the augmented Limb X-ray dataset, which contained both the original dataset and realistic synthetic images made by the AC-GAN. The final hip fracture classification results of the Pelvic X-ray dataset showed that our generative model not only succeeded in producing realistic Limb X-ray data but also helped improve the performance of the transfer learning model for hip fracture detection.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72814-666-9},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\965HCJYD\Lin and Chung - 2019 - Medical Data Augmentation Using Generative Adversa.pdf}
}

@inproceedings{liuDataAugmentationUsing2022,
  title = {Data {{Augmentation Using Image-to-image Translation}} for {{Tongue Coating Thickness Classification}} with {{Imbalanced Data}}},
  booktitle = {2022 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  author = {Liu, Mingxuan and Jiao, Yunrui and Gu, Hongyu and Lu, Jingqiao and Chen, Hong},
  year = {2022},
  month = oct,
  pages = {90--94},
  publisher = {IEEE},
  address = {Taipei, Taiwan},
  doi = {10.1109/BioCAS54905.2022.9948645},
  urldate = {2024-05-13},
  abstract = {Tongue diagnosis is widely used in traditional Chinese medicine diagnosis. The classification of tongue coating thickness is one of the most important tasks in tongue diagnosis. However, data imbalance imposes challenges when using deep learning methods for tongue coating thickness classification. In this paper, we propose a data augmentation method using image-to-image translation to solve the data imbalance problem. First, we use an image-to-image translation model based on generative adversarial networks (GANs) to translate thick and thin tongue coating images into each other, then we train the classification model using synthetic images together with real images. Finally, the trained classification model is used to classify the thickness of tongue coating. With our data augmentation method, the classification performance yields 0.92 accuracy and 0.922 F1-score, which is 3.37\% and 3.95\% higher than that with re-sampling method respectively.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-917-3},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\X9VAVAPH\Liu et al. - 2022 - Data Augmentation Using Image-to-image Translation.pdf}
}

@article{liuSyntheticDataAugmentation2022,
  title = {Synthetic {{Data Augmentation Using Multiscale Attention CycleGAN}} for {{Aircraft Detection}} in {{Remote Sensing Images}}},
  author = {Liu, Weixing and Luo, Bin and Liu, Jun},
  year = {2022},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {19},
  pages = {1--5},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2021.3052017},
  urldate = {2024-05-13},
  abstract = {Deep learning approaches require enough training samples to perform well, but it is a challenge to collect enough real training data and label them manually. In this letter, we propose a practical framework for automatically generating content-rich synthetic images with ground-truth annotations. By rendering 3-D CAD models, we generate two synthetic aircraft image data sets with wide distribution (Syn N and Syn U). For improving the quality of synthetic images, we propose a multiscale attention module which enhances the Cycle-Consistent Adversarial Network (CycleGAN) in spatial and channel dimensions. Then, we compare the synthetic images before and after translation qualitatively and quantitatively. Experiments on Northwestern Polytechnical University (NWPU) very high resolution (VHR)-10, University of Chinese Academy of Sciences, orientation robust object detection in aerial images (UCASAOD), and benchmark for object DetectIon in Optical Remote sensing images (DIOR) data sets demonstrate that synthetic data augmentation can improve the performance of aircraft detection in remote sensing images, especially when real data are insufficient. Synthetic data are available at: https://weix-liu. github.io/.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\6FS4RS5U\Liu et al. - 2022 - Synthetic Data Augmentation Using Multiscale Atten.pdf}
}

@inproceedings{luoSYNTHETICMINORITYCLASS2020,
  title = {{{SYNTHETIC MINORITY CLASS DATA BY GENERATIVE ADVERSARIAL NETWORK FOR IMBALANCED SAR TARGET RECOGNITION}}},
  booktitle = {{{IGARSS}} 2020 - 2020 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Luo, Zhongming and Jiang, Xue and Liu, Xingzhao},
  year = {2020},
  month = sep,
  pages = {2459--2462},
  issn = {2153-7003},
  doi = {10.1109/IGARSS39084.2020.9323439},
  urldate = {2024-05-14},
  abstract = {The deep convolutional neural networks (CNNs) have achieved the state of art performance in synthetic aperture radar (SAR) automatic target recognition (ATR). However, these networks often provide sub-optimal recognition results in the case of imbalanced SAR data distribution. In this paper, a synthetic minority class data method for improving imbalanced SAR target recognition using the generative adversarial network (GAN) is proposed. The minority class SAR data is first over-sampled by optimized data augmentation policies from automatic search method, which enlarge the training set for GAN. The progressive growing of GANs (PGGAN) is then trained on these data and generates high quality and diverse minority class SAR data to alleviate imbalanced data distribution. Experimental results on the designed imbalanced distributed Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset indicate that our method can effectively improve the recognition accuracy of minority class by approximately 11.68\%.},
  keywords = {Gallium nitride,generative adversarial network,Generative adversarial networks,Image resolution,imbalanced target recognition,SAR,Synthetic aperture radar,Target recognition,Training,Training data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\3RR66CD8\\Luo et al. - 2020 - SYNTHETIC MINORITY CLASS DATA BY GENERATIVE ADVERS.pdf;C\:\\Users\\maila\\Zotero\\storage\\VU6MBAZN\\9323439.html}
}

@inproceedings{masengiInvestigatingDeepLearning2023,
  title = {Investigating {{Deep Learning}} for {{Classification}} of {{Psychiatric Disorders}} on {{Brain MRI Data Using GAN-Based Augmentation}}},
  booktitle = {2023 {{International Conference}} on {{Electrical}} and {{Information Technology}} ({{IEIT}})},
  author = {Masengi, Gracia Angelina Jeniffer and Sardjono, Tri and Purnomo, Mauridhi Hery},
  year = {2023},
  month = sep,
  pages = {231--236},
  publisher = {IEEE},
  address = {Malang, Indonesia},
  doi = {10.1109/IEIT59852.2023.10335528},
  urldate = {2024-05-14},
  abstract = {One of the difficulties in diagnosing patients in the field of psychiatry is the lack of objective biomarker measurements. In general, diagnosis is based on traditional methods relying on behavioral symptoms. Currently, diagnostic approaches predominantly rely on traditional methods grounded in behavioral symptoms. The evolution of neuroimaging techniques has prompted extensive investigations into biological indicators of psychiatric disorders. Among these endeavors, the application of deep learning for classification has demonstrated substantial promise. Through generative models like the generative adversarial network (GAN), deep learning is being investigated more in the field of medical image analysis to address the problems of small data samples and class imbalance. In this study, deep learning is implemented for automatic detection of psychiatric diseases from brain magnetic resonance imaging (MRI). Convolutional neural networks (CNN) are used for the diagnosis to categorize brain images into one of the classes: (1) normal, (2) schizophrenia, and (3) bipolar disorder. Image synthesis as data augmentation was performed with GAN. The suggested approach aims to advance the state-of-theart in psychiatric diagnosis and promote additional investigation into the use of brain imaging in diagnostic plans. The findings show that the amount of data and subject variability significantly affect the efficacy of approaches for diagnosing psychiatric disorders. In this study, the data scarcity problem CNN training was not successfully resolved by GANbased augmentation. It is determined that in order to develop a successful automatic psychiatric disease diagnostic system, a sufficient data sample must be collected while taking into account variables like subject variability.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350327298},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\ISWAKDES\Masengi et al. - 2023 - Investigating Deep Learning for Classification of .pdf}
}

@inproceedings{moonStudyNeRFbasedSynthetic2022,
  title = {A {{Study}} on {{NeRF-based Synthetic Image Generation}} and {{Post-processing Method}} for {{Object Detection}}},
  booktitle = {2022 13th {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  author = {Moon, SungWon and Lee, Jiwon and Lee, Jungsoo and Nam, Dowon and Yoo, Wonyoung},
  year = {2022},
  month = oct,
  pages = {1560--1562},
  issn = {2162-1241},
  doi = {10.1109/ICTC55196.2022.9952798},
  urldate = {2024-05-14},
  abstract = {Recently, the importance of securing data in the field of computer vision using machine learning is increasing. In this situation, research to use synthetic images as training data in fields where real images cannot be used for various reasons is ongoing. In particular, in the defense field, where it is difficult to know the characteristics of imaging devices, neural network learning using synthetic images is essential for the introduction of civilian technology. In this paper, we propose a method to use for object detection neural network training by generating synthetic images instead of real images and removing artifacts from the generated images.},
  keywords = {data augmentation,Image synthesis,Imaging,Machine learning,Neural networks,neural radiance fields,object detection,Object detection,Training,Training data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\GS3P3Y7V\\Moon et al. - 2022 - A Study on NeRF-based Synthetic Image Generation a.pdf;C\:\\Users\\maila\\Zotero\\storage\\W38EVC3I\\9952798.html}
}

@inproceedings{nSyntheticVertebralColumn2021,
  title = {Synthetic {{Vertebral Column Fracture Image Generation}} by {{Deep Convolution Generative Adversarial Networks}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Electronics}}, {{Computing}} and {{Communication Technologies}} ({{CONECCT}})},
  author = {N, Sindhura D and Pai, Radhika M and Bhat, Shyamasunder N and M, Manohara Pai M},
  year = {2021},
  month = jul,
  pages = {1--4},
  issn = {2766-2101},
  doi = {10.1109/CONECCT52877.2021.9622527},
  urldate = {2024-05-09},
  abstract = {In the field of medical imaging, the challenging objective is to generate synthetic, realistic images which resembles the original images. The generated synthetic images would enhance the accuracy of the computer-assisted classification, Decision Support System, which aid the doctor in diagnosis of diseases. The Generative Adversarial Networks (GANs), is a method of data augmentation which can be used to generate synthetic realistic looking images, however low quality images are generated. For AI models, it is challenging tasks to do classification using this low quality images. In this work, generation of high quality synthetic medical image using Deep Convolutional Generative Adversarial Networks (DCGANs) is presented. Data augmentation method by DCGANs is illustrated on the limited dataset of CT (Computed Tomography) images of vertebral column fracture. A total of 340 CT scan images were taken for the study, which comprises of complete burst fracture scans of vertebral column. The evaluation of the generated images was done with Visual Turing Test.},
  keywords = {AI models,Computational modeling,computed tomography,Computed tomography,data augmentation,Deep Convolutional Generative Adversarial Network,Generative adversarial networks,Image synthesis,Medical services,Training data,vertebral column fracture,Visual Turing Test,Visualization},
  file = {C\:\\Users\\maila\\Zotero\\storage\\VJGQCEI3\\N et al. - 2021 - Synthetic Vertebral Column Fracture Image Generati.pdf;C\:\\Users\\maila\\Zotero\\storage\\H4V2MNJ8\\9622527.html}
}

@inproceedings{ohConstructingVesselDataset2021,
  title = {On {{Constructing Vessel Dataset Structure Using GAN-based Data Augmentation}}},
  booktitle = {2021 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  author = {Oh, Ah Reum and Lee, Jiwon and Moon, Sung-Won and Lee, Jung Soo and Nam, Do-Won and Yoo, Wonyoung},
  year = {2021},
  month = oct,
  pages = {1700--1702},
  issn = {2162-1233},
  doi = {10.1109/ICTC52510.2021.9620827},
  urldate = {2024-05-14},
  abstract = {Conventional methods using classical image processing techniques is to limitedly augment basic data for extension of the dataset volume in deep learning network system. A new proposed approach using synthetic data by 3D virtual model and Generative Adversarial Networks (GAN) is able to resolve the lack of dataset adequately, and the performance of the dataset structure can be verified by a classification network model. The single and combined data groups with various types of images were constructed for the accuracy comparison of classification system, and it indicated that the proposal has an appropriate profit for improvement of the system. The composed dataset using data augmentation methods can be applied on both academic and industrial field which have little actual data for deep leaning network systems. Further work will aim to improve the quality of the data from GAN and find the relevant quantity of dataset according to data type.},
  keywords = {Data augmentation,Data construction,Data models,Generative adversarial network,Generative adversarial networks,Image resolution,Image synthesis,Information and communication technology,Solid modeling,Style-transferring,Three-dimensional displays},
  file = {C\:\\Users\\maila\\Zotero\\storage\\HMKTMDAL\\Oh et al. - 2021 - On Constructing Vessel Dataset Structure Using GAN.pdf;C\:\\Users\\maila\\Zotero\\storage\\A2899GE2\\9620827.html}
}

@article{posilovicGenerativeAdversarialNetwork2021,
  title = {Generative Adversarial Network with Object Detector Discriminator for Enhanced Defect Detection on Ultrasonic {{B-scans}}},
  author = {Posilovi{\'c}, Luka and Medak, Duje and Suba{\v s}i{\'c}, Marko and Budimir, Marko and Lon{\v c}ari{\'c}, Sven},
  year = {2021},
  month = oct,
  journal = {Neurocomputing},
  volume = {459},
  pages = {361--369},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.06.094},
  urldate = {2024-05-13},
  abstract = {Non-destructive testing is a set of techniques for defect detection in materials. While the set of imaging techniques is manifold, ultrasonic imaging is the one used the most. The analysis is mainly performed by human inspectors manually analyzing the acquired images. A low number of defects in real ultrasonic inspections and legal issues concerning data from such inspections make it difficult to obtain proper results from automatic ultrasonic image (B-scan) analysis. The goal of presented research is to obtain an improvement of the detection results by expanding the training data set with realistic synthetic samples. In this paper, we present a novel deep learning Generative Adversarial Network model for generating realistic ultrasonic B-scans with defects in distinct locations. Furthermore, we show that generated B-scans can be used for synthetic data augmentation, and can improve the performances of deep convolutional neural object detection networks. Our novel method was developed on a dataset with almost 4000 images and more than 6000 annotated defects. When trained only on real data, detector can achieve an average precision of 70\%. By training only on generated data the results increased to 72\%, and by mixing generated and real data we achieve almost 76\% average precision. We believe that synthetic data generation can generalize to other tasks with limited data. It could also be used for training human personnel.},
  keywords = {Automated defect detection,Generative adversarial networks,Image generation,Non-destructive testing,Ultrasonic B-scan},
  file = {C\:\\Users\\maila\\Zotero\\storage\\5SC77247\\Posiloviƒá et al. - 2021 - Generative adversarial network with object detecto.pdf;C\:\\Users\\maila\\Zotero\\storage\\ENKY2A5J\\S0925231221010304.html}
}

@inproceedings{randoDCGANbasedMedicalImage2022,
  title = {{{DCGAN-based Medical Image Augmentation}} to {{Improve ELM Classification Performance}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Communication}}, {{Networks}} and {{Satellite}} ({{COMNETSAT}})},
  author = {{Rando} and Setiawan, Noor Akhmad and Permanasari, Adhistya Erna and Rulaningtyas, Riries and Suksmono, Andriyan Bayu and Sitanggang, Imas Sukaesih},
  year = {2022},
  month = nov,
  pages = {206--211},
  publisher = {IEEE},
  address = {Solo, Indonesia},
  doi = {10.1109/COMNETSAT56033.2022.9994559},
  urldate = {2024-05-19},
  abstract = {Cervical cancer is one of the deadliest diseases in women. One of the cervical cancer screening methods is pap smear method. However, using a pap smear method to detect cervical cancer takes a long time for a pathologist to diagnose. Hence, a rapid development of medical computerization for early detection to get the results quickly is needed. This paper proposes synthetic data augmentation by using Deep Convolutional Generative Adversarial Network (DCGAN) to increase number of pap smear samples in dataset. Gray Level Co-occurrence Matrix (GLCM) is employed to extract features from dataset. Classification of 3 classes which are Adenocarcinoma, High-Grade Squamous Intraepithelial Lesion (HSIL), and Squamous Cell Carcinoma (SCC) is conducted using Extreme Learning Machine (ELM). The result shows that the addition of synthetic data improves the performance of ELM with the accuracy of 90\%. This accuracy is better than the accuracy of ELM using only the original dataset which is 85\%.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-030-9},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\GMBMWETK\Rando et al. - 2022 - DCGAN-based Medical Image Augmentation to Improve .pdf}
}

@article{rozanecSyntheticDataAugmentation2023,
  title = {Synthetic {{Data Augmentation Using GAN For Improved Automated Visual Inspection}}},
  author = {Ro{\v z}anec, Jo{\v z}e M. and Zajec, Patrik and Theodoropoulos, Spyros and Koehorst, Erik and Fortuna, Bla{\v z} and Mladeni{\'c}, Dunja},
  year = {2023},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {22nd {{IFAC World Congress}}},
  volume = {56},
  number = {2},
  pages = {11094--11099},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2023.10.817},
  urldate = {2024-05-09},
  abstract = {Quality control is a crucial activity manufacturing companies perform to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers' discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can reduce the labeling workload if tolerating some labeling errors. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898. We performed the research with real-world data provided by Philips Consumer Lifestyle BV.},
  keywords = {Advanced manufacturing,Data Augmentation,Industry 4.0,Intelligent manufacturing systems,Manufacturing plant control,Quality Inspection,Smart Manufacturing,Visual Inspection},
  file = {C\:\\Users\\maila\\Zotero\\storage\\HMECD2JH\\Ro≈æanec et al. - 2023 - Synthetic Data Augmentation Using GAN For Improved.pdf;C\:\\Users\\maila\\Zotero\\storage\\ABHYXWI8\\S2405896323011941.html}
}

@inproceedings{sasmalImprovedEndoscopicPolyp2020,
  title = {Improved {{Endoscopic Polyp Classification}} Using {{GAN Generated Synthetic Data Augmentation}}},
  booktitle = {2020 {{IEEE Applied Signal Processing Conference}} ({{ASPCON}})},
  author = {Sasmal, Pradipta and Bhuyan, M.K. and Sonowal, Sourav and Iwahori, Yuji and Kasugai, Kunio},
  year = {2020},
  month = oct,
  pages = {247--251},
  doi = {10.1109/ASPCON49795.2020.9276732},
  urldate = {2024-05-09},
  abstract = {Early diagnosis of cancer in polyps detected in the endoscopic video frames helps in better prognosis and clinical management. For this, the polyp regions are exhaustively analyzed by an endoscopist. In this paper, an automated polyp classifier in a deep learning framework is proposed. As the availability of ground truth data for colonic polyp is always in paucity, data augmentation is indispensable in such task. Our work proposes the use of Generative adversial networks (GANs) for synthetic data generation. For classification, a CNN is trained which discriminate between normal (benign) and cancer (malignanat) polyps. Experiments carried on two databases prove that the proposed data augmentation technique can efficiently be used in the classification of colonic polyps. Also, our proposed method compares the performance achieved using classical augmentation approach which is generally considered in limited data scenario. Experimental results show that the classification accuracy is competitive to the state-of-the-art methods.},
  keywords = {Cancer,CNN,Convolution,Data augmentation,Deep learning,Generative adversarial network,Generative adversarial networks,Generators,Medical diagnostic imaging,Polyp classification,Training},
  file = {C:\Users\maila\Zotero\storage\XBC6Y4FJ\Sasmal et al. - 2020 - Improved Endoscopic Polyp Classification using GAN.pdf}
}

@inproceedings{sedighGeneratingSyntheticMedical2019,
  title = {Generating {{Synthetic Medical Images}} by {{Using GAN}} to {{Improve CNN Performance}} in {{Skin Cancer Classification}}},
  booktitle = {2019 7th {{International Conference}} on {{Robotics}} and {{Mechatronics}} ({{ICRoM}})},
  author = {Sedigh, Pooyan and Sadeghian, Rasoul and Masouleh, Mehdi Tale},
  year = {2019},
  month = nov,
  pages = {497--502},
  issn = {2572-6889},
  doi = {10.1109/ICRoM48714.2019.9071823},
  urldate = {2024-05-14},
  abstract = {One of the main reasons of slow progress in using deep learning methods for cancer detection is the lack of data, especially the annotated data which is usually used for supervised learning algorithms. This paper presents a Convolutional Neural Network (CNN) to detect skin cancer. The primary database which is used to train the designed CNN algorithm has 97 members (50 benign and 47 malignant), which are collected from the International Skin Imaging Collaboration (ISIC). In order to compensate the lack of data for training the proposed CNN algorithm, a Generative Adversarial Network (GAN) is designed to produce synthetic skin cancer images. The classification performance of the designed trained CNN without the obtained synthetic images is near 53\%, but by adding the synthetic images to the primary database the performance of the model is increased to 71\%.},
  keywords = {Artificial Intelligence,Convlutional Neural Networks,Databases,Gallium nitride,Generative Adversarial Network,Generative adversarial networks,Medical diagnostic imaging,Skin cancer,Skin Cancer Detection},
  file = {C\:\\Users\\maila\\Zotero\\storage\\DZ68YC5U\\Sedigh et al. - 2019 - Generating Synthetic Medical Images by Using GAN t.pdf;C\:\\Users\\maila\\Zotero\\storage\\7YTS4L3J\\9071823.html}
}

@inproceedings{singhDataAugmentationSpectrally2022,
  title = {Data {{Augmentation Through Spectrally Controlled Adversarial Networks}} for {{Classification}} of {{Multispectral Remote Sensing Images}}},
  booktitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Singh, Abhishek and Bruzzone, Lorenzo},
  year = {2022},
  month = jul,
  pages = {651--654},
  issn = {2153-7003},
  doi = {10.1109/IGARSS46834.2022.9884928},
  urldate = {2024-05-14},
  abstract = {Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics.},
  keywords = {Analytical models,Classification Accuracy,Convolutional neural networks,Data Augmentation,Deep learning,Generative adversarial networks,Generative Model,Measurement,Remote Sensing Image Analysis,Sensors,Training,Training Data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\DI76VXAW\\Singh and Bruzzone - 2022 - Data Augmentation Through Spectrally Controlled Ad.pdf;C\:\\Users\\maila\\Zotero\\storage\\A2AY25DV\\9884928.html}
}

@inproceedings{tanGANbasedMedicalImage2023,
  title = {{{GAN-based Medical Image Augmentation}} for {{Improving CNN Performance}} in {{Myositis Ultrasound Image Classification}}},
  booktitle = {2023 6th {{International Conference}} on {{Electronics Technology}} ({{ICET}})},
  author = {Tan, Hao and Lang, Xun and He, Bingbing and Lu, Yu and Zhang, Yufeng},
  year = {2023},
  month = may,
  pages = {1329--1333},
  publisher = {IEEE},
  address = {Chengdu, China},
  doi = {10.1109/ICET58434.2023.10211926},
  urldate = {2024-05-19},
  abstract = {Idiopathic inflammatory myopathies (IIMs) are a clinically heterogeneous group of immune-mediated inflammatory muscle diseases characterized by muscle weakness and multisystem involvement. Early diagnosis of IIMs can contribute to better prognosis and clinical management. The classifier based on a deep learning framework has proven to be an effective and noninvasive technique for classifying IIMs from ultrasound images. However, as the availability of clinical ultrasound data for IIMs is always in paucity, data augmentation is indispensable to improve the classification accuracy. To this end, this work proposes the use of generative adversarial networks (GAN) with attentional mechanism to generate IIMs ultrasound data. We employ VGG16 with transfer learning as a classification network and find that the proposed method improves by 1.10\% compared to no data enhancement, reaching 92.23\% on the validation dataset. It is also shown that with the same classification network, the data augmentation by GAN is more competitive in facilitating classification compared to the traditional data augmentation.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350337693},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\NV9LMNA3\Tan et al. - 2023 - GAN-based Medical Image Augmentation for Improving.pdf}
}

@inproceedings{viertelPollenGANSyntheticPollen2021,
  title = {{{PollenGAN}}: {{Synthetic Pollen Grain Image Generation}} for {{Data Augmentation}}},
  shorttitle = {{{PollenGAN}}},
  booktitle = {2021 20th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Viertel, Philipp and Konig, Matthias and Rexilius, Jan},
  year = {2021},
  month = dec,
  pages = {44--49},
  publisher = {IEEE},
  address = {Pasadena, CA, USA},
  doi = {10.1109/ICMLA52953.2021.00015},
  urldate = {2024-05-13},
  abstract = {Palynology, the study of pollen, is becoming the focus of attention in computer vision in recent years. Various proposed automated classification and segmentation methods have been evaluated on a number of data sets. However, as of 2021 most data sets are sparse; they either contain only a small number of pollen classes, images in total or are imbalanced overall. In this work, we explore the possibility of creating synthetic pollen grain images from less than 2,000 images per pollen class via a Generative Adversarial Network (GAN). For that purpose, we selected two distinct pollen classes from a state of the art pollen data set and evaluated the data set with and without synthetic data on a Convolutional Neural Network (CNN). The enriched data set performed better overall (+1.4\%) and specifically for the two pollen classes (+2\%). We also drastically reduced the no. of real images and were still able to achieve a score of 60\% to 80\%. The experiments show, that our synthesized pollen images are visually close to real-life pollen grains and can be used to enrich imbalanced data sets as an addition to traditional data augmentation methods.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66544-337-1},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\TBB68ESX\Viertel et al. - 2021 - PollenGAN Synthetic Pollen Grain Image Generation.pdf}
}

@article{wuGPTPromptControlledDiffusion2024,
  title = {{{GPT-Prompt Controlled Diffusion}} for {{Weakly-Supervised Semantic Segmentation}}},
  author = {Wu, Wangyu and Dai, Tianhong and Huang, Xiaowei and Ma, Fei and Xiao, Jimin},
  year = {2024},
  month = jan,
  abstract = {Weakly supervised semantic segmentation (WSSS), aiming to train segmentation models solely using image-level labels, has received significant attention. Existing approaches mainly concentrate on creating high-quality pseudo labels by utilizing existing images and their corresponding image-level labels. However, the quality of pseudo labels degrades significantly when the size of available dataset is limited. Thus, in this paper, we tackle this problem from a different view by introducing a novel approach called GPT-Prompt Controlled Diffusion (GPCD) for data augmentation. This approach enhances the current labeled datasets by augmenting with a variety of images, achieved through controlled diffusion guided by GPT prompts. In this process, the existing images and image-level labels provide the necessary control information, where GPT is employed to enrich the prompts, leading to the generation of diverse backgrounds. Moreover, we integrate data source information as tokens into the Vision Transformer (ViT) framework. These tokens are specifically designed to improve the ability of downstream WSSS framework to recognize the origins of augmented images. Our proposed GPCD approach clearly surpasses existing state-of-the-art methods. This effect is more obvious when the amount of available data is small, demonstrating the effectiveness of our method. Our source code will be released.},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\YVH9XUL5\Wu et al. - GPT-Prompt Controlled Diffusion for Weakly-Supervi.pdf}
}

@article{xieGANBasedSubInstanceAugmentation2024,
  title = {{{GAN-Based Sub-Instance Augmentation}} for {{Open-Pit Mine Change Detection}} in {{Remote Sensing Images}}},
  author = {Xie, Zilin and Jiang, Jinbao and Yuan, Deshuai and Li, Kangning and Liu, Ziwei},
  year = {2024},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {62},
  pages = {1--19},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2023.3336658},
  urldate = {2024-05-14},
  abstract = {Remote sensing (RS) change detection (CD) for open-pit mines plays a critical role in both mineral development and environmental conservation. The performance of supervised deep-learning-based CD for open-pit mines is often limited by the amount of available data, necessitating data augmentation. Current CD data augmentation methods mainly focus on image-, region-, and instance-level transformations. However, mining area changes occur at a finer sub-instance level, leading to suboptimal performance of existing methods. Therefore, this study proposes a novel data augmentation method named generative adversarial network (GAN)-based sub-instance augmentation (GSIA). This method enables the generation of realistic and diverse CD samples using unchanged data from mining areas to address the issue of data scarcity in open-pit mine CD. GSIA comprises three steps. In the first two steps, GSIA achieves sub-instance-level transformations by sequentially applying GAN-based local editing to the labels and images of the mining areas. In the third step, GSIA constructs a synthetic CD dataset by randomly combining the bitemporal data. The effectiveness of GSIA was evaluated by comparing it with 14 other data augmentation methods on five CD models, and GSIA outperformed all of them. In addition, training solely on synthetic data generated by GSIA achieved an overall accuracy of 97.64\% and an F1 -score of 78.65\%, which were comparable to training with all available real data. Furthermore, GSIA can address the insufficient correlation between the training and test sets in domain adaptation. GSIA plays a significant guiding role in the data augmentation for open-pit mine CD. Our code is available at https://github.com/ZilinCode/GSIA.},
  keywords = {Change detection (CD),data augmentation,Data augmentation,Data mining,Data models,Feature extraction,generative adversarial networks (GANs),Image synthesis,open-pit mine,Remote sensing,sub-instance-level transformation,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\KRZMBRA2\\Xie et al. - 2024 - GAN-Based Sub-Instance Augmentation for Open-Pit M.pdf;C\:\\Users\\maila\\Zotero\\storage\\XDFL7ZDZ\\10329996.html}
}

@inproceedings{xuSemiSupervisedAttentionGuidedCycleGAN2019,
  title = {Semi-{{Supervised Attention-Guided CycleGAN}} for {{Data Augmentation}} on {{Medical Images}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Xu, Zhenghua and Qi, Chang and Xu, Guizhi},
  year = {2019},
  month = nov,
  pages = {563--568},
  doi = {10.1109/BIBM47256.2019.8982932},
  urldate = {2024-05-14},
  abstract = {Recently, deep learning methods, in particular, convolutional neural networks (CNNs), have made a massive breakthrough in computer vision. And a big amount of annotated data is the essential cornerstone to reach this success. However, in the medical domain, it is usually difficult (and sometimes even impossible) to get sufficient data for some specific learning tasks. Consequently, in this work, a novel data augmentation solution, called semi-supervised attention-guided CycleGAN (SSA-CycleGAN) is proposed to resolve this problem. Specifically, a cycle-consistency GANs-based model is first proposed to generate synthetic tumor (resp., normal) images from normal (resp., tumor) images. Then, a semi-supervised attention module is further proposed to enhance the model's capability in learning the important details of the training images, which in turns help the generated synthetic images become more realistic. To verify its effectiveness, experimental studies are conducted on three medical image datasets with limited amounts of MRI images, and the proposed SSA-CycleGAN is applied to generate synthetic tumor and normal MRI images for data augmentation. Experimental results show that i) SSA-CycleGAN can add (resp., remove) tumor lesions on (resp., from) the original normal (resp., tumor) images and generate very realistic synthetic tumor (resp. normal) images; and ii) in the ResNet18-based MRI image classification tasks based on these datasets, data augmentation using SSA-CycleGAN achieves much better classification performances than the classic data augmentation methods.},
  keywords = {Attention module,Data Augmentation,Generative adversarial networks (GANs),Medical image processing},
  file = {C\:\\Users\\maila\\Zotero\\storage\\HIGAPTGL\\Xu et al. - 2019 - Semi-Supervised Attention-Guided CycleGAN for Data.pdf;C\:\\Users\\maila\\Zotero\\storage\\XLK9W4LF\\8982932.html}
}

@article{youngchoiAutomatedDetectionCrystalline2023a,
  title = {Automated Detection of Crystalline Retinopathy via Fundus Photography Using Multistage Generative Adversarial Networks},
  author = {Young Choi, Eun and Han, Seung Hoon and Hee Ryu, Ik and Kuk Kim, Jin and Sik Lee, In and Han, Eoksoo and Kim, Hyungsu and Yul Choi, Joon and Keun Yoo, Tae},
  year = {2023},
  month = oct,
  journal = {Biocybernetics and Biomedical Engineering},
  volume = {43},
  number = {4},
  pages = {725--735},
  issn = {0208-5216},
  doi = {10.1016/j.bbe.2023.10.005},
  urldate = {2024-05-22},
  abstract = {Purpose Crystalline retinopathy is characterized by reflective crystal deposits in the macula and is caused by various systemic conditions including hereditary, toxic, and embolic etiologies. Herein, we introduce a novel application of deep learning with a multistage generative adversarial network (GAN) to detect crystalline retinopathy using fundus photography. Methods The dataset comprised major classes (healthy retina, diabetic retinopathy, exudative age-related macular degeneration, and drusen) and a crystalline retinopathy class (minor set). To overcome the limited data on crystalline retinopathy, we proposed a novel multistage GAN framework. The GAN was retrained after CutMix combination by inputting the GAN-generated synthetic data as new inputs to the original training data. After the multistage CycleGAN augmented the data for crystalline retinopathy, we built a deep-learning classifier model for detection. Results Using the multistage CycleGAN facilitated realistic fundus photography synthesis with the characteristic features of retinal crystalline deposits. The proposed method outperformed typical transfer learning, prototypical networks, and knowledge distillation for both multiclass and binary classifications. The final model achieved an area under the curve of the receiver operating characteristics of 0.962 for internal validation and 0.987 for external validation for the detection of crystalline retinopathy. Conclusion We introduced a deep learning approach for detecting crystalline retinopathy, a potential biomarker of underlying systemic pathological conditions. Our approach enables realistic pathological image synthesis and more accurate prediction of crystalline retinopathy, an essential but minor retinal condition.},
  keywords = {Crystalline retinopathy,CutMix,Deep learning,Fundus photography,Multistage GAN},
  file = {C:\Users\maila\Downloads\1-s2.0-S0208521623000591-main.pdf}
}

@inproceedings{zhuangFMRIDataAugmentation2019,
  title = {{{FMRI Data Augmentation Via Synthesis}}},
  booktitle = {2019 {{IEEE}} 16th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2019)},
  author = {Zhuang, Peiye and Schwing, Alexander G. and Koyejo, Oluwasanmi},
  year = {2019},
  month = apr,
  pages = {1783--1787},
  publisher = {IEEE},
  address = {Venice, Italy},
  doi = {10.1109/ISBI.2019.8759585},
  urldate = {2024-05-14},
  abstract = {We present an empirical evaluation of fMRI data augmentation via synthesis. For synthesis we use generative models trained on real neuroimaging data to produce novel taskdependent functional brain images. Analyzed generative models include classic approaches such as the Gaussian mixture model (GMM), and modern implicit generative models such as the generative adversarial network (GAN) and the variational autoencoder (VAE). In particular, the proposed GAN and VAE models utilize 3-dimensional convolutions, which enables modeling of high-dimensional brain image tensors with structured spatial correlations. The synthesized datasets are then used to augment classifiers designed to predict cognitive and behavioural outcomes. Our results suggest that the proposed models are able to generate high-quality synthetic brain images which are diverse and task-dependent. Perhaps most importantly, the performance improvements of data augmentation via synthesis are shown to be complementary to the choice of the predictive model. Thus, our results suggest that data augmentation via synthesis is a promising approach to address the limited availability of fMRI data, and to improve the quality of predictive fMRI models.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-5386-3641-1},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\N6TFFJLU\Zhuang et al. - 2019 - FMRI Data Augmentation Via Synthesis.pdf}
}

@article{zhuDataAugmentationUsing2020,
  title = {Data Augmentation Using Improved {{cDCGAN}} for Plant Vigor Rating},
  author = {Zhu, Fengle and He, Mengzhu and Zheng, Zengwei},
  year = {2020},
  month = aug,
  journal = {Computers and Electronics in Agriculture},
  volume = {175},
  pages = {105603},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2020.105603},
  urldate = {2024-05-22},
  abstract = {The supervised deep learning models rely on large labeled training samples, which is a common challenge affecting current plant phenotyping studies. One practical approach to alleviate the insufficient training samples is data augmentation. In this study, we investigated the data augmentation approach using improved cDCGAN (conditional deep convolutional generative adversarial network) for vigor rating of orchid seedlings, a significant but labor-intensive task in modern commercial greenhouse. Various modifications on the architecture of cDCGAN network were explored for generating high-quality fine-grained RGB plant images with designated class labels. ResNet deep learning classifier was employed for performance evaluation throughout the whole analysis. On the small training sets, which obtained obviously worse ResNet classification results than bigger sets, cDCGAN was employed to generate additional plant images. The synthesized images provided a significant boost in classification performance, up to a 0.23 increase in the testing F1 score after data augmentation, achieving comparable results with that obtained with larger training sets without data augmentation. Different size of real and augmented training sets for optimal classification was systematically evaluated. The advantage of the improved cDCGAN architecture with added bypass connections was also demonstrated. The proposed data augmentation approach might be extended to deal with the common challenge of insufficient data size in other plant science tasks.},
  keywords = {cDCGAN,Data augmentation,Deep learning,Plant vigor rating},
  file = {C:\Users\maila\Zotero\storage\TJCFFTNR\S016816992030106X.html}
}

@article{zhuDataAugmentationUsing2020a,
  title = {Data Augmentation Using Improved {{cDCGAN}} for Plant Vigor Rating},
  author = {Zhu, Fengle and He, Mengzhu and Zheng, Zengwei},
  year = {2020},
  month = aug,
  journal = {Computers and Electronics in Agriculture},
  volume = {175},
  pages = {105603},
  issn = {01681699},
  doi = {10.1016/j.compag.2020.105603},
  urldate = {2024-05-22},
  abstract = {The supervised deep learning models rely on large labeled training samples, which is a common challenge affecting current plant phenotyping studies. One practical approach to alleviate the insufficient training samples is data augmentation. In this study, we investigated the data augmentation approach using improved cDCGAN (conditional deep convolutional generative adversarial network) for vigor rating of orchid seedlings, a significant but labor-intensive task in modern commercial greenhouse. Various modifications on the architecture of cDCGAN network were explored for generating high-quality fine-grained RGB plant images with designated class labels. ResNet deep learning classifier was employed for performance evaluation throughout the whole analysis. On the small training sets, which obtained obviously worse ResNet classification results than bigger sets, cDCGAN was employed to generate additional plant images. The synthesized images provided a significant boost in classification performance, up to a 0.23 increase in the testing F1 score after data augmentation, achieving comparable results with that obtained with larger training sets without data augmentation. Different size of real and augmented training sets for optimal classification was systematically evaluated. The advantage of the improved cDCGAN architecture with added bypass connections was also demonstrated. The proposed data augmentation approach might be extended to deal with the common challenge of insufficient data size in other plant science tasks.},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\MRVEMXQ6\Zhu et al. - 2020 - Data augmentation using improved cDCGAN for plant .pdf}
}

@inproceedings{choiFaceRecognitionSoldiers2024,
  title = {Face {{Recognition}} for {{Soldiers}} with {{Bulletproof Helmets Using Generative Networks}} and {{Data Augmentation}}},
  booktitle = {2024 {{International Conference}} on {{Electronics}}, {{Information}}, and {{Communication}} ({{ICEIC}})},
  author = {Choi, Minseo and Shin, Wunjong and Pyo, Heesu and Song, Wootaek and Jung, Byungjun and Shin, Minwoo and Paik, Joonki},
  year = {2024},
  month = jan,
  pages = {1--2},
  issn = {2767-7699},
  doi = {10.1109/ICEIC61013.2024.10457279},
  urldate = {2024-05-14},
  abstract = {Face recognition (FR) is a key technology for a wide range of applications, including privacy-preserving security, education, entertainment, and emotion recognition. However, existing FR methods require a large amount of data to train, and their performance can be degraded by factors such as bulletproof helmets worn by soldiers. This paper proposes a new FR method for soldiers with bulletproof helmets that uses generative networks and data augmentation techniques to effectively increase the amount of training data. The proposed method first generates synthetic face images of soldiers with bulletproof helmets using a generative network. Then, it uses data augmentation techniques to further increase the diversity of the training data. The proposed method is evaluated on both our own dataset of soldiers with bulletproof helmets and a public dataset. The results show that the proposed method outperforms existing FR methods on both datasets, especially in the presence of bulletproof helmets. Overall, the proposed method is a promising new approach for FR for soldiers with bulletproof helmets. It can be used to improve the performance of FR systems in military applications, such as soldier identification and authentication.},
  keywords = {Data augmentation,Data Augmentation,Emotion recognition,Entertainment industry,Face recognition,Face Recognition,Generative Model,Head,Supervised learning,Training data},
  file = {C\:\\Users\\maila\\Zotero\\storage\\PP6RQEQM\\Choi et al. - 2024 - Face Recognition for Soldiers with Bulletproof Hel.pdf;C\:\\Users\\maila\\Zotero\\storage\\5UIMKXX8\\10457279.html}
}

@article{deptoQuantifyingImbalancedClassification2023,
  title = {Quantifying Imbalanced Classification Methods for Leukemia Detection},
  author = {Depto, Deponker Sarker and Rizvee, Md. Mashfiq and Rahman, Aimon and Zunair, Hasib and Rahman, M. Sohel and Mahdy, M. R. C.},
  year = {2023},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {152},
  pages = {106372},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2022.106372},
  urldate = {2024-05-14},
  abstract = {Uncontrolled proliferation of B-lymphoblast cells is a common characterization of Acute Lymphoblastic Leukemia (ALL). B-lymphoblasts are found in large numbers in peripheral blood in malignant cases. Early detection of the cell in bone marrow is essential as the disease progresses rapidly if left untreated. However, automated classification of the cell is challenging, owing to its fine-grained variability with B-lymphoid precursor cells and imbalanced data points. Deep learning algorithms demonstrate potential for such fine-grained classification as well as suffer from the imbalanced class problem. In this paper, we explore different deep learning-based State-Of-The-Art (SOTA) approaches to tackle imbalanced classification problems. Our experiment includes input, GAN (Generative Adversarial Networks), and loss-based methods to mitigate the issue of imbalanced class on the challenging C-NMC and ALLIDB-2 dataset for leukemia detection. We have shown empirical evidence that loss-based methods outperform GAN-based and input-based methods in imbalanced classification scenarios.},
  keywords = {Adversarial training,Domain adaptation,Imbalanced classification,Leukemia classification},
  file = {C:\Users\maila\Zotero\storage\D97W9967\S0010482522010800.html}
}

@inproceedings{desaiBreastCancerDetection2020,
  title = {Breast {{Cancer Detection Using GAN}} for {{Limited Labeled Dataset}}},
  booktitle = {2020 12th {{International Conference}} on {{Computational Intelligence}} and {{Communication Networks}} ({{CICN}})},
  author = {Desai, Shrinivas D and Giraddi, Shantala and Verma, Nitin and Gupta, Puneet and Ramya, Sharan},
  year = {2020},
  month = sep,
  pages = {34--39},
  issn = {2472-7555},
  doi = {10.1109/CICN49253.2020.9242551},
  urldate = {2024-05-14},
  abstract = {Mammography is the primary procedure for breast cancer screening, attempting to reduce breast cancer mortality risk with early detection. Deep learning methods have shown strong applicability to various medical images datasets. Due to paucity of available labeled medical images, accurate computer assisted diagnosis requires intensive data augmentation (DA) techniques, such as geometric/intensity transformations of original images. This data when used along with the training data helps to address the limited medical image dataset collected from various sources. Generative Adversarial Networks (GANs) is one of the DA techniques. GAN trained on images can generate new images that contain many authentic characteristics and look realistic to human observers. Therefore, this paper focuses on overcoming the problem of limited labeled dataset, using Deep Convolution GANs (DCGANs). To analyze the closeness between the original and synthetic images, a visualization tool ImageJ was used. In order to validate the proposed model, a visual Turing test was conducted with the help of medical experts.},
  keywords = {Breast cancer,Data augmentation,DCGAN,Generative adversarial networks,Image Profile,Mammography,Medical diagnostic imaging,Synthetic Image,Tools,Training,Training data,Visual Turing Test,Visualization},
  file = {C:\Users\maila\Zotero\storage\9C37R7P8\Desai et al. - 2020 - Breast Cancer Detection Using GAN for Limited Labe.pdf}
}

@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128{\texttimes}128, 4.59 on ImageNet 256{\texttimes}256, and 7.72 on ImageNet 512{\texttimes}512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256{\texttimes}256 and 3.85 on ImageNet 512{\texttimes}512. We release our code at https://github.com/openai/guided-diffusion.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\maila\Zotero\storage\N4GW6DJR\Dhariwal e Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@misc{dolatabadiDevilAdvocateShattering2024,
  title = {The {{Devil}}'s {{Advocate}}: {{Shattering}} the {{Illusion}} of {{Unexploitable Data}} Using {{Diffusion Models}}},
  shorttitle = {The {{Devil}}'s {{Advocate}}},
  author = {Dolatabadi, Hadi M. and Erfani, Sarah and Leckie, Christopher},
  year = {2024},
  month = jan,
  number = {arXiv:2303.08500},
  eprint = {2303.08500},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08500},
  urldate = {2024-05-14},
  abstract = {Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\maila\\Zotero\\storage\\HDKMT8F7\\Dolatabadi et al. - 2024 - The Devil's Advocate Shattering the Illusion of U.pdf;C\:\\Users\\maila\\Zotero\\storage\\N5IW3Q2D\\2303.html}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\maila\Zotero\storage\KJZMMBLQ\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf}
}

@article{iglesiasSurveyGANsComputer2023,
  title = {A Survey on {{GANs}} for Computer Vision: {{Recent}} Research, Analysis and Taxonomy},
  shorttitle = {A Survey on {{GANs}} for Computer Vision},
  author = {Iglesias, Guillermo and Talavera, Edgar and {D{\'i}az-{\'A}lvarez}, Alberto},
  year = {2023},
  month = may,
  journal = {Computer Science Review},
  volume = {48},
  pages = {100553},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2023.100553},
  urldate = {2024-05-22},
  abstract = {In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will be analyzed. The final objective of this survey is to provide a summary of the evolution and performance of the GANs which are having better results to guide future researchers in the field.},
  keywords = {Artificial intelligence,Deep learning,Generative Adversarial Network,Machine learning},
  file = {C\:\\Users\\maila\\Zotero\\storage\\EM2GJ8Y3\\Iglesias et al. - 2023 - A survey on GANs for computer vision Recent resea.pdf;C\:\\Users\\maila\\Zotero\\storage\\MIT8L2RP\\Iglesias et al. - 2023 - A survey on GANs for computer vision Recent resea.pdf;C\:\\Users\\maila\\Zotero\\storage\\3US25CUC\\S1574013723000205.html}
}

@misc{ImprovingRobustnessPedestrian,
  title = {Improving the {{Robustness}} of {{Pedestrian Detection}} in {{Autonomous Driving}} with {{Generative Data Augmentation}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-05-14},
  howpublished = {https://ieeexplore.ieee.org/document/10438990},
  file = {C:\Users\maila\Zotero\storage\X8RYCR6Z\10438990.html}
}

@inproceedings{karrasProgressiveGrowingGANs2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  month = feb,
  urldate = {2024-05-22},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\6YTQPB77\Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf}
}

@inproceedings{khoslaEnhancingPerformanceDeep2020,
  title = {Enhancing {{Performance}} of {{Deep Learning Models}} with Different {{Data Augmentation Techniques}}: {{A Survey}}},
  shorttitle = {Enhancing {{Performance}} of {{Deep Learning Models}} with Different {{Data Augmentation Techniques}}},
  booktitle = {2020 {{International Conference}} on {{Intelligent Engineering}} and {{Management}} ({{ICIEM}})},
  author = {Khosla, Cherry and Saini, Baljit Singh},
  year = {2020},
  month = jun,
  pages = {79--85},
  doi = {10.1109/ICIEM48762.2020.9160048},
  urldate = {2024-05-14},
  abstract = {Deep convolutional neural networks have shown impressive results on different computer vision tasks. Nowadays machines are fed by new artificial intelligence techniques which makes them intelligent enough to cognize the visual world better than humans. These computer vision algorithms rely heavily on large data sets. Having a large training data set plays a very crucial role in the performance of deep convolutional neural networks. We can enhance the performance of the model by augmenting the data of the image. Data augmentation is a set of techniques that are used to increase the size and quality of the image with label preserving transformations. This survey paper focuses on different data augmentation techniques based on data warping and oversampling. In addition to data augmentation techniques, this paper gives a brief discussion on different solutions of reducing the overfitting problem.},
  keywords = {Colored noise,Computer vision,Convolutional neural networks,data augmentation,Data models,data warping,deep learning models,Image color analysis,Machine learning,overfitting,synthetic oversampling,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\45DDSPWK\\Khosla and Saini - 2020 - Enhancing Performance of Deep Learning Models with.pdf;C\:\\Users\\maila\\Zotero\\storage\\3UBTZ3VZ\\9160048.html}
}

@inproceedings{luDataAugmentationMethod2019,
  title = {Data {{Augmentation Method}} of {{SAR Image Dataset Based}} on {{Wasserstein Generative Adversarial Networks}}},
  booktitle = {2019 {{International Conference}} on {{Electronic Engineering}} and {{Informatics}} ({{EEI}})},
  author = {Lu, Qinglin and Jiang, Haiyang and Li, Guojing and Ye, Wei},
  year = {2019},
  month = nov,
  pages = {488--490},
  doi = {10.1109/EEI48997.2019.00111},
  urldate = {2024-05-14},
  abstract = {The published Synthetic Aperture Radar (SAR) samples are not abundant enough, which is not conducive to the application of deep learning methods in the field of SAR automatic target recognition. Generative Adversarial Nets (GANs) is one of the most effective ways to generate image samples. In this manuscript, the gradient penalty method was used on the traditional DCGAN to solve the mode collapse problem. The improved model was used for data augmentation of the MSTAR dataset. All the experimental results show that the method was an effective way to generate SAR images with high quality.},
  keywords = {component,data augmentation,Generative Adversarial Nets,gradient penalty},
  file = {C\:\\Users\\maila\\Zotero\\storage\\8FCLMI3F\\Lu et al. - 2019 - Data Augmentation Method of SAR Image Dataset Base.pdf;C\:\\Users\\maila\\Zotero\\storage\\G7NC6GA5\\8991042.html}
}

@inproceedings{maSARTargetRecognition2019,
  title = {{{SAR Target Recognition Based}} on {{Transfer Learning}} and {{Data Augmentation}} with {{LSGANs}}},
  booktitle = {2019 {{Chinese Automation Congress}} ({{CAC}})},
  author = {Ma, Yu and Liang, Yan and Zhang, WanYing and Yan, Shi},
  year = {2019},
  month = nov,
  pages = {2334--2337},
  issn = {2688-0938},
  doi = {10.1109/CAC48633.2019.8996717},
  urldate = {2024-05-14},
  abstract = {This paper presents a method combining transfer learning and data augmentation based on least squares generative adversarial networks (LSGANs), which can effectively solve the problem of lacking training samples in the synthetic aperture radar (SAR) target recognition algorithm. Compared with the conventional methods of data augmentation, the new generated data are more realistic when using the LSGANs to expand the data set, and the recognition accuracy is significantly improved. Transfer learning can apply characteristics extracted from the source domain for the target domain, which effectively reduce the need for training samples in existing algorithms. Features were extracted by using ResNet50 that has been pre-trained, and convolution neural network (CNN) was used as a classifier for SAR target recognition. Experimental results based on real radar data sets testify that the method proposed in this paper significantly improves the recognition accuracy.},
  keywords = {data augmentation,Least squares generative adversarial networks (LSGANs),Machine-to-machine communications,synthetic aperture radar (SAR),target recognition,transfer learning},
  file = {C\:\\Users\\maila\\Zotero\\storage\\KMB98IQH\\Ma et al. - 2019 - SAR Target Recognition Based on Transfer Learning .pdf;C\:\\Users\\maila\\Zotero\\storage\\6YF7IE5Q\\8996717.html}
}

@article{meisterInvestigationsExplainableArtificial2021,
  title = {Investigations on {{Explainable Artificial Intelligence}} Methods for the Deep Learning Classification of Fibre Layup Defect in the Automated Composite Manufacturing},
  author = {Meister, Sebastian and Wermes, Mahdieu and St{\"u}ve, Jan and Groves, Roger M.},
  year = {2021},
  month = nov,
  journal = {Composites Part B: Engineering},
  volume = {224},
  pages = {109160},
  issn = {1359-8368},
  doi = {10.1016/j.compositesb.2021.109160},
  urldate = {2024-05-22},
  abstract = {Automated fibre layup techniques are widely used in the aviation sector for the efficient production of composite components. However, the required manual inspection can take up to 50~\% of the manufacturing time. The automated classification of fibre layup defects with Neural Networks potentially increases the inspection efficiency. However, the machine decision-making processes of such classifiers are difficult to verify. Hence, we present an approach for analysing the classification procedure of fibre layup defects. Therefore, we comprehensively evaluate 20 Explainable Artificial Intelligence methods from the literature. Accordingly, the techniques Smoothed Integrated Gradients, Guided Gradient Class Activation Mapping and DeepSHAP are applied to a Convolutional Neural Network classifier. These methods analyse the neural activations and robustness of a classifier for an unknown and manipulated input data. Our investigations show that especially Smoothed Integrated Gradients and DeepSHAP are well suited for the visualisation of such classifications. Additionally, maximum-sensitivity and infidelity calculations confirm this behaviour. In future, customers and developers could apply the presented methods for the certification of their inspection systems.},
  keywords = {Automation,Defects,Non-destructive testing,Process monitoring},
  file = {C:\Users\maila\Zotero\storage\GH6DSG8T\Meister et al. - 2021 - Investigations on Explainable Artificial Intellige.pdf}
}

@article{singhSIGANSpectralIndex2022,
  title = {{{SIGAN}}: {{Spectral Index Generative Adversarial Network}} for {{Data Augmentation}} in {{Multispectral Remote Sensing Images}}},
  shorttitle = {{{SIGAN}}},
  author = {Singh, Abhishek and Bruzzone, Lorenzo},
  year = {2022},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {19},
  pages = {1--5},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2021.3093238},
  urldate = {2024-05-14},
  abstract = {Generative models are typically employed to approximate the distribution of deep features. Recently, these state-of-the-art methods have been applied to estimate image transformations by an unsupervised learning approach. In this letter, a novel spectral index generative adversarial network (SIGAN) is proposed for the generation of multispectral (MS) remote sensing images. This network is defined to effectively perform data augmentation starting from a limited number of training samples in the MS remote sensing domain for training deep learning models. The SIGAN model is able to capture class-specific properties in data augmentation, by incorporating the task-specific normalized spectral indices to model class-by-class properties of MS images. Experimental results obtained on a Sentinel 2 dataset show that the proposed model provides better performance than other generative adversarial networks (GANs) in MS data generation.},
  keywords = {Convolution,Data augmentation,Data models,Generative adversarial networks,Generators,Indexes,labeled sample,multispectral (MS) images,remote sensing,Remote sensing,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\I7HQWQ4Q\\Singh and Bruzzone - 2022 - SIGAN Spectral Index Generative Adversarial Netwo.pdf;C\:\\Users\\maila\\Zotero\\storage\\8A5MVSBQ\\9537161.html}
}

@article{tiagoDataAugmentationPipeline2022,
  title = {A {{Data Augmentation Pipeline}} to {{Generate Synthetic Labeled Datasets}} of {{3D Echocardiography Images Using}} a {{GAN}}},
  author = {Tiago, Cristiana and Gilbert, Andrew and Beela, Ahmed Salem and Aase, Svein Arne and Snare, Sten Roar and Sprem, Jurica and McLeod, Kristin},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {98803--98815},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3207177},
  urldate = {2024-05-14},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\BMXMFA9J\Tiago et al. - 2022 - A Data Augmentation Pipeline to Generate Synthetic.pdf}
}

@article{wangDiffMDDDiffusionBasedDeep2024,
  title = {{{DiffMDD}}: {{A Diffusion-Based Deep Learning Framework}} for {{MDD Diagnosis Using EEG}}},
  shorttitle = {{{DiffMDD}}},
  author = {Wang, Yilin and Zhao, Sha and Jiang, Haiteng and Li, Shijian and Luo, Benyan and Li, Tao and Pan, Gang},
  year = {2024},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {32},
  pages = {728--738},
  issn = {1558-0210},
  doi = {10.1109/TNSRE.2024.3360465},
  urldate = {2024-05-14},
  abstract = {Major Depression Disorder (MDD) is a common yet destructive mental disorder that affects millions of people worldwide. Making early and accurate diagnosis of it is very meaningful. Recently, EEG, a non-invasive technique of recording spontaneous electrical activity of brains, has been widely used for MDD diagnosis. However, there are still some challenges in data quality and data size of EEG: (1) A large amount of noise is inevitable during EEG collection, making it difficult to extract discriminative features from raw EEG; (2) It is difficult to recruit a large number of subjects to collect sufficient and diverse data for model training. Both of the challenges cause the overfitting problem, especially for deep learning methods. In this paper, we propose DiffMDD, a diffusion-based deep learning framework for MDD diagnosis using EEG. Specifically, we extract more noise-irrelevant features to improve the model's robustness by designing the Forward Diffusion Noisy Training Module. Then we increase the size and diversity of data to help the model learn more generalized features by designing the Reverse Diffusion Data Augmentation Module. Finally, we re-train the classifier on the augmented dataset for MDD diagnosis. We conducted comprehensive experiments to test the overall performance and each module's effectiveness. The framework was validated on two public MDD diagnosis datasets, achieving the state-of-the-art performance.},
  keywords = {Brain modeling,Data models,deep learning,Deep learning,Diffusion processes,electroencephalogram,Electroencephalography,Feature extraction,Major depression disorder,mental disorder diagnosis,Training},
  file = {C\:\\Users\\maila\\Zotero\\storage\\ZADWZ9ZP\\Wang et al. - 2024 - DiffMDD A Diffusion-Based Deep Learning Framework.pdf;C\:\\Users\\maila\\Zotero\\storage\\IH3GJVTU\\10417777.html}
}

@inproceedings{wuImageAugmentationControlled2024,
  title = {Image {{Augmentation}} with {{Controlled Diffusion}} for {{Weakly-Supervised Semantic Segmentation}}},
  booktitle = {{{ICASSP}} 2024 - 2024 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Wangyu and Dai, Tianhong and Huang, Xiaowei and Ma, Fei and Xiao, Jimin},
  year = {2024},
  month = apr,
  pages = {6175--6179},
  issn = {2379-190X},
  doi = {10.1109/ICASSP48485.2024.10447893},
  urldate = {2024-05-14},
  abstract = {Weakly-supervised semantic segmentation (WSSS), which aims to train segmentation models solely using image-level labels, has achieved significant attention. Existing methods primarily focus on generating high-quality pseudo labels using available images and their image-level labels. However, the quality of pseudo labels degrades significantly when the size of available dataset is limited. Thus, in this paper, we tackle this problem from a different view by introducing a novel approach called Image Augmentation with Controlled Diffusion (IACD). This framework effectively augments existing labeled datasets by generating diverse images through controlled diffusion, where the available images and image-level labels are served as the controlling information. Moreover, we also propose a high-quality image selection strategy to mitigate the potential noise introduced by the randomness of diffusion models. In the experiments, our proposed IACD approach clearly surpasses existing state-of-the-art methods. This effect is more obvious when the amount of available data is small, demonstrating the effectiveness of our method.},
  keywords = {Data augmentation,Data models,diffusion model,high-quality image selection,Image augmentation,Semantic segmentation,Semantics,Signal processing,Training,weakly-supervised semantic segmentation},
  file = {C\:\\Users\\maila\\Zotero\\storage\\2GA6KPXJ\\Wu et al. - 2024 - Image Augmentation with Controlled Diffusion for W.pdf;C\:\\Users\\maila\\Zotero\\storage\\V29SARE2\\10447893.html}
}

@article{yangDiffusionModelsComprehensive2023,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2023},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {4},
  pages = {105:1--105:39},
  issn = {0360-0300},
  doi = {10.1145/3626235},
  urldate = {2024-05-30},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
  keywords = {diffusion models,Generative models,score-based generative models,stochastic differential equations},
  file = {C:\Users\maila\Zotero\storage\MZSSGLBS\Yang et al. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf}
}


@article{youngchoiAutomatedDetectionCrystalline2023,
  title = {Automated Detection of Crystalline Retinopathy via Fundus Photography Using Multistage Generative Adversarial Networks},
  author = {Young Choi, Eun and Han, Seung Hoon and Hee Ryu, Ik and Kuk Kim, Jin and Sik Lee, In and Han, Eoksoo and Kim, Hyungsu and Yul Choi, Joon and Keun Yoo, Tae},
  year = {2023},
  month = oct,
  journal = {Biocybernetics and Biomedical Engineering},
  volume = {43},
  number = {4},
  pages = {725--735},
  issn = {0208-5216},
  doi = {10.1016/j.bbe.2023.10.005},
  urldate = {2024-05-13},
  abstract = {Purpose Crystalline retinopathy is characterized by reflective crystal deposits in the macula and is caused by various systemic conditions including hereditary, toxic, and embolic etiologies. Herein, we introduce a novel application of deep learning with a multistage generative adversarial network (GAN) to detect crystalline retinopathy using fundus photography. Methods The dataset comprised major classes (healthy retina, diabetic retinopathy, exudative age-related macular degeneration, and drusen) and a crystalline retinopathy class (minor set). To overcome the limited data on crystalline retinopathy, we proposed a novel multistage GAN framework. The GAN was retrained after CutMix combination by inputting the GAN-generated synthetic data as new inputs to the original training data. After the multistage CycleGAN augmented the data for crystalline retinopathy, we built a deep-learning classifier model for detection. Results Using the multistage CycleGAN facilitated realistic fundus photography synthesis with the characteristic features of retinal crystalline deposits. The proposed method outperformed typical transfer learning, prototypical networks, and knowledge distillation for both multiclass and binary classifications. The final model achieved an area under the curve of the receiver operating characteristics of 0.962 for internal validation and 0.987 for external validation for the detection of crystalline retinopathy. Conclusion We introduced a deep learning approach for detecting crystalline retinopathy, a potential biomarker of underlying systemic pathological conditions. Our approach enables realistic pathological image synthesis and more accurate prediction of crystalline retinopathy, an essential but minor retinal condition.},
  keywords = {Crystalline retinopathy,CutMix,Deep learning,Fundus photography,Multistage GAN},
  file = {C:\Users\maila\Zotero\storage\FTS2CG5C\S0208521623000591.html}
}


@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {C:\Users\maila\Zotero\storage\8TWG3ZMV\Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016},
  keywords = {Machine learning},
  file = {C:\Users\maila\Zotero\storage\LXBVSFPM\Ian Goodfellow_ Yoshua Bengio_ Aaron Courville - Deep Learning (2016).pdf}
}

%% Livros
@book{haykinNeuralNetworksLearning2009,
  title = {Neural Networks and Learning Machines},
  author = {Haykin, Simon S.},
  year = {2009},
  edition = {3rd ed},
  publisher = {Prentice Hall},
  address = {New York},
  isbn = {978-0-13-147139-9},
  langid = {english},
  lccn = {QA76.87 .H39 2009},
  keywords = {Adaptive filters,Neural networks (Computer science)},
  annotation = {OCLC: ocn237325326},
  file = {C:\Users\maila\Zotero\storage\SZTI5G6S\Haykin e Haykin - 2009 - Neural networks and learning machines.pdf}
}

@book{russellArtificialIntelligenceModern2016,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  edition = {Third edition, Global edition},
  publisher = {Pearson},
  address = {Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo},
  collaborator = {Davis, Ernest and Edwards, Douglas},
  isbn = {978-0-13-604259-4 978-1-292-15396-4},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\WUDRPKBH\Russell e Norvig - 2016 - Artificial intelligence a modern approach.pdf}
}

@book{szeliskiComputerVisionAlgorithms2011,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2011},
  series = {Texts in {{Computer Science}}},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-84882-935-0},
  urldate = {2024-05-27},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-1-84882-934-3 978-1-84882-935-0},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\TZRH2WUC\Szeliski - 2011 - Computer Vision Algorithms and Applications.pdf}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2024-05-28},
  langid = {english},
  file = {C:\Users\maila\Zotero\storage\HM7C3BYZ\Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@article{millerExplanationArtificialIntelligence2019,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2018.07.007},
  urldate = {2024-05-30},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency},
  file = {C\:\\Users\\maila\\Zotero\\storage\\WM7ES6PJ\\Miller - 2019 - Explanation in artificial intelligence Insights f.pdf;C\:\\Users\\maila\\Zotero\\storage\\54RHAGGT\\S0004370218305988.html}
}


@article{banymuhammadEigenCAMVisualExplanations2021,
  title = {Eigen-{{CAM}}: {{Visual Explanations}} for {{Deep Convolutional Neural Networks}}},
  shorttitle = {Eigen-{{CAM}}},
  author = {Bany Muhammad, Mohammed and Yeasin, Mohammed},
  year = {2021},
  month = jan,
  journal = {SN Computer Science},
  volume = {2},
  number = {1},
  pages = {47},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00449-3},
  urldate = {2024-05-30},
  abstract = {The adoption of deep convolutional neural networks (CNN) is growing exponentially in wide varieties of applications due to exceptional performance that equals to or is better than classical machine learning as well as a human. However, such models are difficult to interpret, susceptible to overfit, and hard to decode failure. An increasing body of literature, such as class activation map (CAM), focused on understanding what representations or features a model learned from the data. This paper presents novel Eigen-CAM to enhance explanations of CNN predictions by visualizing principal components of learned representations from convolutional layers. The Eigen-CAM is intuitive, easy to use, computationally efficient, and does not require correct classification by the model. Eigen-CAM can work with all CNN models without the need to modify layers or retrain models. For the task of generating a visual explanation of CNN predictions, compared to state-of-the-art methods, Eigen-CAM is more consistent, class discriminative, and robust against classification errors made by dense layers. Empirical analyses and comparison with the best state-of-the-art methods show up to 12\% improvement in weakly-supervised object localization, an average of 13\% improvement in weakly-supervised segmentation, and at least 15\% improvement in generic object proposal.},
  langid = {english},
  keywords = {Class activation maps,Explainable AI,Salient features,Visual explanation of CNN,Weakly supervised localization}
}

@inproceedings{chattopadhayGradCAMGeneralizedGradientBased2018,
  title = {Grad-{{CAM}}++: {{Generalized Gradient-Based Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  year = {2018},
  month = mar,
  pages = {839--847},
  doi = {10.1109/WACV.2018.00097},
  urldate = {2024-05-30},
  abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision based problems. However, deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest to develop explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose Grad-CAM++ to provide better visual explanations of CNN model predictions (when compared to Grad-CAM), in terms of better localization of objects as well as explaining occurrences of multiple objects of a class in a single image. We provide a mathematical explanation for the proposed method, Grad-CAM++, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ indeed provides better visual explanations for a given CNN architecture when compared to Grad-CAM.},
  keywords = {Heating systems,Machine learning,Mathematical model,Neurons,Predictive models,Visualization},
  file = {C:\Users\maila\Zotero\storage\BVBZ47EI\Chattopadhay et al. - 2018 - Grad-CAM++ Generalized Gradient-Based Visual Expl.pdf}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-30},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {C:\Users\maila\Zotero\storage\R2QXPRS8\Lundberg e Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2024-05-30},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning},
  file = {C:\Users\maila\Zotero\storage\XXS5FP3N\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  urldate = {2024-05-30},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization},
  file = {C\:\\Users\\maila\\Zotero\\storage\\P6M96JQN\\Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\maila\\Zotero\\storage\\AYQ47FLG\\8237336.html}
}

@article{figueiraSurveySyntheticData2022,
  title = {Survey on {{Synthetic Data Generation}}, {{Evaluation Methods}} and {{GANs}}},
  author = {Figueira, Alvaro and Vaz, Bruno},
  year = {2022},
  month = jan,
  journal = {Mathematics},
  volume = {10},
  number = {15},
  pages = {2733},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math10152733},
  urldate = {2024-05-30},
  abstract = {Synthetic data consists of artificially generated data. When data are scarce, or of poor quality, synthetic data can be used, for example, to improve the performance of machine learning models. Generative adversarial networks (GANs) are a state-of-the-art deep generative models that can generate novel synthetic samples that follow the underlying data distribution of the original dataset. Reviews on synthetic data generation and on GANs have already been written. However, none in the relevant literature, to the best of our knowledge, has explicitly combined these two topics. This survey aims to fill this gap and provide useful material to new researchers in this field. That is, we aim to provide a survey that combines synthetic data generation and GANs, and that can act as a good and strong starting point for new researchers in the field, so that they have a general overview of the key contributions and useful references. We have conducted a review of the state-of-the-art by querying four major databases: Web of Sciences (WoS), Scopus, IEEE Xplore, and ACM Digital Library. This allowed us to gain insights into the most relevant authors, the most relevant scientific journals in the area, the most cited papers, the most significant research areas, the most important institutions, and the most relevant GAN architectures. GANs were thoroughly reviewed, as well as their most common training problems, their most important breakthroughs, and a focus on GAN architectures for tabular data. Further, the main algorithms for generating synthetic data, their applications and our thoughts on these methods are also expressed. Finally, we reviewed the main techniques for evaluating the quality of synthetic data (especially tabular data) and provided a schematic overview of the information presented in this paper.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {evaluation of synthetic data,generative adversarial networks,synthetic data generation},
  file = {C:\Users\maila\Zotero\storage\5WH588C4\Figueira e Vaz - 2022 - Survey on Synthetic Data Generation, Evaluation Me.pdf}
}

@misc{xraychestdataset,
    title = {Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification},
    author = {Daniel Kermany and Kang Zhang and Michael Goldbaum},
    year = {2018},
    doi = {10.17632/rscbjbr9sj.2}
}

@inproceedings{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}.},
	pages = {6840--6851},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2024-06-01},
	year = {2020},
}